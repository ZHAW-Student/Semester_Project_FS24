---
title: Differentiation of walking patterns
subtitle: An attempt to differntiate walking patterns based on attributes and context information \Project Work for the module Patterns and Trends FS24
author: Saskia Gianola and Sarah Wirth
output: bookdown::html_document2(global_numbering = TRUE)
format:
  html:
    code-fold: true
execute:
  warning: false
  message: false
lang: en  # switch to "de" if you write your report in german
bibliography: bibliography.bib
---
```{r preprocessing_attribute, results='hide', fig.keep='none'}
#| code-summary: Preprocessing for Attribute based classification
# used libraries #### 
library("XML")
library("leaflet")
library("sf")
library("tmap")
library("ggplot2")
library("tidyverse")
library("readr")
library("trajr")
library("yardstick")
library("caret")
library("DiagrammeR")
library("png")
library("DiagrammeRsvg")
library("magrittr")
library("rsvg")
library("readr")
library("dplyr")
library("rpart")
library("stringr")

# Preprocessing ####
# read training activities
training_files <- list.files("activities/.", pattern = "*.gpx")

for (i in 1:length(training_files)){
  filename <- paste0("act", i)
  wd <- paste0("activities/", training_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# function to parse activities and write into data frame
built_df <- function(activity) {
  # get coordinates
  coords <- xpathSApply(doc = activity, path = "//trkpt", fun = xmlAttrs)
  # get elevation
  elevation <- xpathSApply(doc = activity, path = "//trkpt/ele", fun = xmlValue)
  # get time
  time <- xpathApply(doc = activity, path = "//trkpt/time", fun = xmlValue)
  data.frame(
    lat = as.numeric(coords["lat", ]),
    lon = as.numeric(coords["lon", ]),
    ts_POSIXct = ymd_hms(time, tz = "UTC"),
    elevation = as.numeric(elevation)
  )
}
# convert files to dataframe
act1_df <- built_df(act1)
act2_df <- built_df(act2)
act3_df <- built_df(act3)
act4_df <- built_df(act4)
act5_df <- built_df(act5)
act6_df <- built_df(act6)
act7_df <- built_df(act7)
act8_df <- built_df(act8)
act9_df <- built_df(act9)
act10_df <- built_df(act10)
act11_df <- built_df(act11)
act12_df <- built_df(act12)

# assign ID and description to single files
act1_df$ID <- "test_1"
act1_df$ID_text <- "test_Waedenswil_Reidbach_Zentrum"
act2_df$ID <- "test_2"
act2_df$ID_text <- "test_Waedenswil_Coop_Bahnhof"
act3_df$ID <- "test_3"
act3_df$ID_text <- "test_Waedenswil_Schloss_Mensa"
act4_df$ID <- "test_4"
act4_df$ID_text <- "test_Waedenswil_Gruental_Bahnhof1"
act5_df$ID <- "test_5"
act5_df$ID_text <- "test_Waedenswil_Gruental_Bahnhof2"
act6_df$ID <- "test_6"
act6_df$ID_text <- "test_Rueti_Home_Bahnhof_Coop_Home"
act7_df$ID <- "test_7"
act7_df$ID_text <- "test_test_Rapperswil_See_Bahnhof"
act8_df$ID <- "test_8"
act8_df$ID_text <- "test_Rueti_Home_Coop_Home"
act9_df$ID <- "test_9"
act9_df$ID_text <- "test_Neuhausen_Bahnhof_Rhein"
act10_df$ID <- "test_10"
act10_df$ID_text <- "test_Neuhausen_Rhein_Bahnhof"
act11_df$ID <- "test_11"
act11_df$ID_text <- "test_S-chanf"
act12_df$ID <- "test_12"
act12_df$ID_text <- "test_Regensdorf_Buero_Coop"

# function to convert data frame into sf object
df_to_sf <- function(df){
  st_as_sf(df, coords = c("lon", "lat"), crs = 4326 , remove = FALSE)
}

# combine all activitites into one data frame
training_activities_df <- rbind(act1_df, act2_df, act3_df, act4_df, act5_df, act6_df, act7_df, act8_df, act9_df, act10_df, act11_df, act12_df)

# turn data frame into sf object
training_activities_sf <- df_to_sf(training_activities_df)

# export sf object for setting attributes in GIS
# export_test_activities <- st_write(test_activities_sf, "test_activities.shp")

# read Saskias activities
saskia_files <- list.files("activities/activities_saskia.", pattern = "*.gpx")

for (i in 1:length(saskia_files)){
  filename <- paste0("saskia_", i)
  wd <- paste0("activities/activities_saskia/", saskia_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# convert files to dataframe
saskia_1_df <- built_df(saskia_1)
saskia_2_df <- built_df(saskia_2)
saskia_3_df <- built_df(saskia_3)
saskia_4_df <- built_df(saskia_4)
saskia_5_df <- built_df(saskia_5)
saskia_6_df <- built_df(saskia_6)

# assign ID to single files
saskia_1_df$ID <- "saskia__1"
saskia_1_df$ID_text <- "saskia_Pfaeffikon_Seedamm1"
saskia_2_df$ID <- "saskia__2"
saskia_2_df$ID_text <- "saskia_Pfaeffikon_Seedamm2"
saskia_3_df$ID <- "saskia__3"
saskia_3_df$ID_text <- "saskia_Regensdorf_Buero_Bahnhof"
saskia_4_df$ID <- "saskia__4"
saskia_4_df$ID_text <- "saskia_Rueti_Zentrum_Home"
saskia_5_df$ID <- "saskia__5"
saskia_5_df$ID_text <- "saskia_Rueti_Home_Bahnhof_Coop_Home"
saskia_6_df$ID <- "saskia__6"
saskia_6_df$ID_text <- "saskia_Waedenswil_Gruental_Bhf"

# combine all activitites to one data frame
activities_saskia_df <- rbind(saskia_1_df, saskia_2_df, saskia_3_df, saskia_4_df, saskia_5_df, saskia_6_df)

# turn data frame into sf object
activities_saskia_sf <- df_to_sf(activities_saskia_df)

# export sf object for setting attributes in GIS
# export_activities_saskia <- st_write(activities_saskia_sf, "activities_saskia.csv")

# read Sarahs activities
sarah_files <- list.files("activities/activities_sarah", pattern = "*.gpx")

for (i in 1:length(sarah_files)){
  filename <- paste0("sarah_", i)
  wd <- paste0("activities/activities_sarah/", sarah_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# convert files to dataframe
sarah_1_df <- built_df(sarah_1)
sarah_2_df <- built_df(sarah_2)
sarah_3_df <- built_df(sarah_3)
sarah_4_df <- built_df(sarah_4)
sarah_5_df <- built_df(sarah_5)
sarah_6_df <- built_df(sarah_6)
sarah_7_df <- built_df(sarah_7)
sarah_8_df <- built_df(sarah_8)
sarah_9_df <- built_df(sarah_9)
sarah_10_df <- built_df(sarah_10)
sarah_11_df <- built_df(sarah_11)
sarah_12_df <- built_df(sarah_12)

# assign ID and description to single files
sarah_1_df$ID <- "sarah__1"
sarah_1_df$ID_text <- "sarah_Denner_Einkauf"
sarah_2_df$ID <- "sarah__2"
sarah_2_df$ID_text <- "sarah_Klafi_Feld"
sarah_3_df$ID <- "sarah__3"
sarah_3_df$ID_text <- "sarah_Klafi_von_Zug_Heim"
sarah_4_df$ID <- "sarah__4"
sarah_4_df$ID_text <- "sarah_Migros_groesserer_Einkauf"
sarah_5_df$ID <- "sarah__5"
sarah_5_df$ID_text <- "sarah_Migros_kleiner_Einkauf"
sarah_6_df$ID <- "sarah__6"
sarah_6_df$ID_text <- "sarah_Rosengarten_und_Shopping_Winti"
sarah_7_df$ID <- "sarah__7"
sarah_7_df$ID_text <- "sarah_Spaziergang_am_Morgen_Bub"
sarah_8_df$ID <- "sarah__8"
sarah_8_df$ID_text <- "sarah_Spaziergang_Klafi_kleine_Runde"
sarah_9_df$ID <- "sarah__9"
sarah_9_df$ID_text <- "sarah_WB_Heim"
sarah_10_df$ID <- "sarah__10"
sarah_10_df$ID_text <- "sarah_Zu_Bus_Klafi"
sarah_11_df$ID <- "sarah__11"
sarah_11_df$ID_text <- "sarah_Zum_Bus_Bub"
sarah_12_df$ID <- "sarah__12"
sarah_12_df$ID_text <- "sarah_Zum_Zug_Liestal"

# combine all activities to one data frame
activities_sarah_df <- rbind(sarah_1_df, sarah_2_df, sarah_3_df, sarah_4_df, sarah_5_df, sarah_6_df,sarah_7_df,sarah_8_df,sarah_9_df, sarah_10_df, sarah_11_df,sarah_12_df)

# turn data frame into sf object
activities_sarah_sf <- df_to_sf(activities_sarah_df)

#create new row for activity- classification and classify trajectories with only one activity
activities_sarah_sf<-activities_sarah_sf |> 
  mutate(Attribut = case_when(ID_text == "sarah_Denner_Einkauf" ~ "recreation", 
                              ID_text == "sarah_Spaziergang_am_Morgen_Bub"~ "recreation",
                              ID_text == "sarah_Spaziergang_Klafi_kleine_Runde"  ~ "recreation", 
                              ID_text == "sarah_Klafi_von_Zug_Heim"  ~ "travel",
                              ID_text == "sarah_WB_Heim"~ "travel",
                              ID_text == "sarah_Zu_Bus_Klafi"~ "travel",
                              ID_text == "sarah_Zum_Bus_Bub"~ "travel",
                              ID_text == "sarah_Zum_Zug_Liestal"~ "travel",
                              ID_text == "sarah_Klafi_Feld"~ "recreation" ,
                              ID_text == "sarah_Migros_groesserer_Einkauf"~ "recreation",
                              ID_text == "sarah_Migros_kleiner_Einkauf"~ "recreation" ,
                              ID_text == "sarah_Rosengarten_und_Shopping_Winti"~ "recreation"  ))

# export sf object for setting attributes in GIS
# export_activities_sarah <- st_write(activities_sarah_sf, "activities_sarah.csv")
```

```{r preprocessing_CAMA, results='hide', fig.keep='none', eval=FALSE}
#| code-summary: Preprocessing CAMA based classification
# We decide not to let this chunk run as it relies on the swissTLM3 dataset and therefore takes quite long to run. All outputs generated from those steps are included in the code chunk for the CAMA based classification

##Prepare clip data----
gemeindegrenzen <- read_sf("swissBOUNDARIES3D_1_5_LV95_LN02.gpkg","tlm_hoheitsgebiet")

gemeindeselection <-gemeindegrenzen |>
                  filter(name %in% c("Andelfingen","Bubendorf","Freienbach" 
                                     ,"Kleinandelfingen","Liestal" ,"Neuhausen am
                                     Rheinfall","Rapperswil-Jona","Regensdorf","
                                     Rüti","St. Moritz","S-chanf","Wädenswil"
                                     ,"Winterthur","Zuoz"))

##Reading, clipping and exporting clipped layers----
st_layers("SWISSTLM3D_2024_LV95_LN02.gpkg")#see all contents of geopackage

################### Warning! This does take some time.  

gebaeude <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_bauten_gebaeude_footprint")
gebaeude_selection<-st_intersection(gebaeude,gemeindeselection)
rm(gebaeude)
st_write(gebaeude_selection, dsn="CAMA_data/gebaeude_selection.gpkg")

bodenbedeckung <- read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_bb_bodenbedeckung")
bodenbedeckung_selection<-st_intersection(bodenbedeckung,gemeindeselection)
rm(bodenbedeckung)
st_write(bodenbedeckung_selection, dsn="CAMA_data/bodenbedeckung_selection.gpkg")

nutzungsareal <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_areale_nutzungsareal")
nutzungsareal_selection<-st_intersection(nutzungsareal,gemeindeselection)
rm(nutzungsareal)
st_write(nutzungsareal_selection, dsn="CAMA_data/nutzungsareal_selection.gpkg")

strassen <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_strassen_strasse")
strassen_selection<-st_intersection(strassen,gemeindeselection)
rm(strassen)
st_write(strassen_selection, dsn="CAMA_data/strassen_selection.gpkg")

oev <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_oev_haltestelle")
oev_selection<-st_intersection(oev,gemeindeselection)
rm(oev)
st_write(oev_selection, dsn="CAMA_data/oev_selection.gpkg")

```


```{r attribute-based classification, results='hide', fig.keep='none'}
#| code-summary: Attribute based classification

# attribute-based classification for training data ####
# import csv with attributes and convert to sf
training_activities_attributed <- read_delim("test_activities_attributiert.csv", ",")
training_activities_attributed_sf <- df_to_sf(training_activities_attributed)

# change crs of sf and specify attributes
training_activities_attributed_sf <- st_transform(training_activities_attributed_sf, crs = 2056)

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# Calculate attributes 
## calculate time lag  
difftime_secs <- function(later, now){
  as.numeric(difftime(later, now, units = "secs"))
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

## calculate distance between locations
distance_by_element <- function(later, now){
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

## Check plausibility of calculated parameters
plot(training_activities_attributed_sf$timelag_sec)
plot(training_activities_attributed_sf$steplenght)
boxplot(training_activities_attributed_sf$timelag_sec)
boxplot(training_activities_attributed_sf$steplenght)
summary(training_activities_attributed_sf$timelag_sec)
summary(training_activities_attributed_sf$steplenght)

# based on this check, remove all timelag > 5 and steplenght > 5
outliers_training_timelag <- filter(training_activities_attributed_sf, timelag_sec >= 5)
training_activities_attributed_sf <- training_activities_attributed_sf[which(training_activities_attributed_sf$timelag_sec <= 5),]
outliers_training_steplenght <- filter(training_activities_attributed_sf, steplenght >= 5)
training_activities_attributed_sf <- training_activities_attributed_sf[which(training_activities_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(training_activities_attributed_sf$timelag_sec)
plot(training_activities_attributed_sf$steplenght)
boxplot(training_activities_attributed_sf$timelag_sec)
boxplot(training_activities_attributed_sf$steplenght)

# Segmentation
# Specify a temporal window for mean step
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )

# calculate mean step
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) |>
  ungroup()

# calculate mean speed
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# Explore mean step to define threshold 
hist(training_activities_attributed_sf$stepMean)
boxplot(training_activities_attributed_sf$stepMean)
summary(training_activities_attributed_sf$stepMean)

# apply threshold
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
rle_id <- function(vec) {
  x <- rle(vec)$lengths
  as.factor(rep(seq_along(x), times = x))
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static))|> 
  ungroup()

# extract single trajectories
traj1 <- filter(training_activities_attributed_sf, ID == "test_1")
traj2 <- filter(training_activities_attributed_sf, ID == "test_2")
traj3 <- filter(training_activities_attributed_sf, ID == "test_3")
traj4 <- filter(training_activities_attributed_sf, ID == "test_4")
traj5 <- filter(training_activities_attributed_sf, ID == "test_5")
traj6 <- filter(training_activities_attributed_sf, ID == "test_6")
traj7 <- filter(training_activities_attributed_sf, ID == "test_7")
traj8 <- filter(training_activities_attributed_sf, ID == "test_8")
traj9 <- filter(training_activities_attributed_sf, ID == "test_9")
traj10 <- filter(training_activities_attributed_sf, ID == "test_10")
traj11 <- filter(training_activities_attributed_sf, ID == "test_11")
traj12 <- filter(training_activities_attributed_sf, ID == "test_12")


# Summarize by true activity
summary_training <- training_activities_attributed_sf |> 
  group_by(ID) |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))

# summarize per trajectory
stops_traj1 <- traj1 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj2 <- traj2 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj3 <- traj3 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj4 <- traj4 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj5 <- traj5 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj6 <- traj6 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj7 <- traj7 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj8 <- traj8 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj9 <- traj9 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj10 <- traj10 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj11 <- traj11 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj12 <- traj12 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))

# function to calculate acceleration
acceleration <- function(s1, s2, t){
  as.numeric((s2-s1)/(t))
}

# calculate acceleration 
training_activities_attributed_sf <- training_activities_attributed_sf |> 
mutate(acceleration = 
    acceleration(lag(speedMean), speedMean, timelag_sec))

# summarize attributes per attribute factor
summary_training <- training_activities_attributed_sf |> 
  group_by(ID) |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            stop_ratio = not_stops/stops,
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_lenght = length(segment_id)
            ) |> 
  ungroup()

# create new id to group by for classification
training_activities_for_classification <- training_activities_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))
  

# summarize with new id
training_classification <- training_activities_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
            ) |> 
  ungroup()

# apply thresholds for attributes
training_classification <- training_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                              ,  1 ,  0 ))

# create new attribute with activity as character based on applied thresholds
training_classification <- training_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))

# remove points where classification was not possible
training_classification <- na.omit(training_classification)

# define activity as factor
training_classification <- training_classification |> 
  mutate(activity_factor = as.factor(activity)) 
  
# join classified table with sf object
training_activities_classified <- st_join(training_activities_for_classification, training_classification, left = TRUE)

# calculate confusion matrix to test classification
confus_training <-conf_mat(data = training_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# Export csv for other approaches
# st_write(training_activities_classified, "test_activities_with_attributes_new.csv")

# get statistics for confusion matrix 
confusionMatrix(training_activities_classified$Attribute_factor, training_activities_classified$activity_factor)

# attribute-based classification Saskias data ####
# import csv with attributes and convert to sf
activities_saskia_attributed <- read_delim("activities_saskia_attributiert.csv", ",")
activities_saskia_attributed_sf <- df_to_sf(activities_saskia_attributed)

# change crs of sf
activities_saskia_attributed_sf <- st_transform(activities_saskia_attributed_sf, crs = 2056)

# set attributes
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# calculate time lag
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

# calculate distance between locations
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

# Check plausibility of calculated parameters
plot(activities_saskia_attributed_sf$timelag_sec)
plot(activities_saskia_attributed_sf$steplenght)
boxplot(activities_saskia_attributed_sf$timelag_sec)
boxplot(activities_saskia_attributed_sf$steplenght)
summary(activities_saskia_attributed_sf$timelag_sec)
summary(activities_saskia_attributed_sf$steplenght)

# based on this check, remove all timelag > 5 and steplenght > 5
outliers_timelag <- filter(activities_saskia_attributed_sf, timelag_sec >= 5)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf[which(activities_saskia_attributed_sf$timelag_sec <= 5),]
outliers_steplenght <- filter(activities_saskia_attributed_sf, steplenght >= 5)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf[which(activities_saskia_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(activities_saskia_attributed_sf$timelag_sec)
plot(activities_saskia_attributed_sf$steplenght)
boxplot(activities_saskia_attributed_sf$timelag_sec)
boxplot(activities_saskia_attributed_sf$steplenght)


# Segmentation
# Specify a temporal window for mean step
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )
# calculate mean step
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) 

# calculate mean speed
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# apply threshold (same as for test data)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static)) |> 
  ungroup()

# calculate acceleration
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  mutate(acceleration = 
           acceleration(lag(speedMean), speedMean, timelag_sec))

# create new id for classification
activities_saskia_for_classification <- activities_saskia_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))

# group by new id and summarize attributes
saskia_classification <- activities_saskia_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
  ) |> 
  ungroup()

# apply thresholds defined for test data
saskia_classification <- saskia_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                            ,  1 ,  0 ))

# create attribute with activity as text
saskia_classification <- saskia_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))
# remove points that could not be classified
saskia_classification <- na.omit(saskia_classification)

# set classified activity as factor
saskia_classification <- saskia_classification |> 
  mutate(activity_factor = as.factor(activity)) 

# join classified table with sf object
saskia_activities_classified <- st_join(activities_saskia_for_classification, saskia_classification, left = TRUE)

# Export csv for other approaches
#st_write(saskia_activities_classified, "activities_saskia_with_attributes_classified_new.csv")

# generate confusion matrix for data
confus_saskia <-conf_mat(data = saskia_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# get statistics of confusion matrix
confusionMatrix(saskia_activities_classified$Attribute_factor, saskia_activities_classified$activity_factor)

# attribute-based classification Sarahs data ####
# import csv with attributes and convert to sf
activities_sarah_attributed <- read_delim("activities_sarah_attributiert.csv", ",")
activities_sarah_attributed_sf <- df_to_sf(activities_sarah_attributed)

# change crs of sf
activities_sarah_attributed_sf <- st_transform(activities_sarah_attributed_sf, crs = 2056)

# set attributes
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# calculate time lag
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

#calculate distance between locations
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

# Check plausibility of calculated parameters
plot(activities_sarah_attributed_sf$timelag_sec)
plot(activities_sarah_attributed_sf$steplenght)
boxplot(activities_sarah_attributed_sf$timelag_sec)
boxplot(activities_sarah_attributed_sf$steplenght)
summary(activities_sarah_attributed_sf$timelag_sec)
summary(activities_sarah_attributed_sf$steplenght)

# based on this check, all timelag > 5 and steplenght > 5
outliers_timelag <- filter(activities_sarah_attributed_sf, timelag_sec >= 5)
activities_sarah_attributed_sf <- activities_sarah_attributed_sf[which(activities_sarah_attributed_sf$timelag_sec <= 5),]
outliers_steplenght <- filter(activities_sarah_attributed_sf, steplenght >= 5)
activities_sarah_attributed_sf <- activities_sarah_attributed_sf[which(activities_sarah_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(activities_sarah_attributed_sf$timelag_sec)
plot(activities_sarah_attributed_sf$steplenght)
boxplot(activities_sarah_attributed_sf$timelag_sec)
boxplot(activities_sarah_attributed_sf$steplenght)

# Segmentation
# Specify a temporal window for mean step
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )
# calculate mean step
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) 

# calculate mean speed
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# apply threshold defined in test data
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static)) |> 
  ungroup()

# calculate acceleration ####
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  mutate(acceleration = 
           acceleration(lag(speedMean), speedMean, timelag_sec))

# create new id to group for classification
activities_sarah_for_classification <- activities_sarah_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))

# group by newly set id
sarah_classification <- activities_sarah_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
  ) |> 
  ungroup()

# apply defined thresholds
sarah_classification <- sarah_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                            ,  1 ,  0 ))

# get attribute activity as text
sarah_classification <- sarah_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))

# remove points that could not be classified
sarah_classification <- na.omit(sarah_classification)

# set classified activity as factor
sarah_classification <- sarah_classification |> 
  mutate(activity_factor = as.factor(activity)) 

# join classification table with sf object
sarah_activities_classified <- st_join(activities_sarah_for_classification, sarah_classification, left = TRUE)

# Export csv for other approaches
#st_write(sarah_activities_classified, "activities_sarah_with_attributes_classified_new.csv")

# calcualte confusion matrix
confus_sarah <-conf_mat(data = sarah_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# get statitics for confusion matrix
confusionMatrix(sarah_activities_classified$Attribute_factor, sarah_activities_classified$activity_factor)

```

```{r CAMA classification, results='hide', fig.keep='none'}
#| code-summary: CAMA based classification

##Re-read clipped data, remove not- needed columns and filter if required ----
gebaeude_clip<-read_sf("CAMA_data/gebaeude_selection.gpkg")
gebaeude_clip <- gebaeude_clip[,c(1,10,40)]

bodenbedeckung_clip<-read_sf("CAMA_data/bodenbedeckung_selection.gpkg")
bodenbedeckung_clip<-bodenbedeckung_clip[,c(1,10,37)]

nutzungsareal_clip<-read_sf("CAMA_data/nutzungsareal_selection.gpkg")
nutzungsareal_clip<-nutzungsareal_clip[,c(1,11,39)]

strassen_clip<-read_sf("CAMA_data/strassen_selection.gpkg")
#reduce roads to roads associated with recreational activities
strassen_recreational <-strassen_clip |> 
                      filter(objektart %in% c("Ausfahrt","Einfahrt","Zufahrt",
                                              "3m Strasse","1m Weg" ,"2m Weg",
                                              "1m Wegfragment","2m Wegfragment"))
rm(strassen_clip)
strassen_recreational<-strassen_recreational[,c(1,10,51)]

oev_clip<-read_sf("CAMA_data/oev_selection.gpkg")
oev_clip<-oev_clip[,c(1,10,40)]

##Create Buffers ----
bodenbuf <-st_buffer(bodenbedeckung_clip, dist=10)
rm(bodenbedeckung_clip)
nutzungsbuf <-st_buffer(nutzungsareal_clip, dist=10)
rm(nutzungsareal_clip)
strassenbuf <-st_buffer(strassen_recreational, dist=10)
rm(strassen_recreational)
oevbuf <-st_buffer(oev_clip, dist=50)
rm(oev_clip)

##Workflow with Saskia's training- data ----
###Read activity data ----
sas_tra_activities_classified_sf <-read_sf("CAMA_data/test_activities_attributiert.gpkg")

sas_tra_activities_classified_sf <- st_transform(sas_tra_activities_classified_sf, crs = 2056)

###Join with objects -----
sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, bodenbuf, join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_boden= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, nutzungsbuf ,  join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, strassenbuf, join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_oev= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, gebaeude_clip ,join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_geb= objektart)

sas_tra_activities_classified_sf = subset(sas_tra_activities_classified_sf,
                                  select = -c(uuid.x...9,uuid.y...11, uuid.x...13, uuid.y...15,uuid ))

###Create presence/absence information for objects ----
sas_tra_activities_classified_sf$recreation_b <- if_else(is.na(sas_tra_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf$recreation_n <- if_else(is.na(sas_tra_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf$recreation_s <- if_else(is.na(sas_tra_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE", 
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sas_tra_activities_classified_sf$recreation[is.na(sas_tra_activities_classified_sf$recreation)] <- "FALSE" 

sas_tra_activities_classified_sf$recreation<-as.logical(sas_tra_activities_classified_sf$recreation)

sas_tra_activities_classified_sf$oev <- if_else(is.na(sas_tra_activities_classified_sf$obj_oev== TRUE)
                                       , FALSE, TRUE)

sas_tra_activities_classified_sf$gebaeude <- if_else(is.na(sas_tra_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf[,c(1:8,17:19)]

### Classification ----
sas_tra_classification <- sas_tra_activities_classified_sf |> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                    if_else(oev == TRUE, "travel",
                    if_else(recreation == TRUE, "recreation", "travel"),NA)))

sas_tra_classification <- sas_tra_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sas_tra_classification <- sas_tra_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sas_tra_test_classification, dsn="CAMA_data/ cama_classification_results_saskia_training.gpkg")#export classification results

###Confusion matrix ----
sas_tra_classification <-na.omit(sas_tra_classification)
confus_sas_tra_cama <-conf_mat(data = sas_tra_classification, truth = Attribute_factor, 
                  estimate = activity_factor)

autoplot(confus_sas_tra_cama, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

###Compute model accuracy ----
confusionMatrix(sas_tra_classification$Attribute_factor, sas_tra_classification$activity_factor)

##Workflow with Saskia's test- data ----
###read activity data  ----
sas_tes_activities_classified_sf <-read_csv("CAMA_data/activities_saskia_attributiert.csv")
sas_tes_activities_classified_sf$ts_POSIXct <-as.POSIXct(sas_tes_activities_classified_sf$ts_POSIXct)

sas_tes_activities_classified_sf <-st_as_sf(sas_tes_activities_classified_sf, 
                          coords = c("lon", "lat"), crs = 4326 , remove = FALSE) 

sas_tes_activities_classified_sf <- st_transform(sas_tes_activities_classified_sf, crs = 2056)

###Join with objects -----
sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, bodenbuf,join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_boden= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, nutzungsbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, strassenbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_oev= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, gebaeude_clip , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_geb= objektart)

sas_tes_activities_classified_sf = subset(sas_tes_activities_classified_sf,
                          select = -c(fid,uuid.x...10,uuid.y...12, uuid.x...14,
                                      uuid.y...16,uuid))

###Create presence/absence information for objects ----
sas_tes_activities_classified_sf$recreation_b <- if_else(is.na(sas_tes_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$recreation_n <- if_else(is.na(sas_tes_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$recreation_s <- if_else(is.na(sas_tes_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE",
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sas_tes_activities_classified_sf$recreation[is.na(sas_tes_activities_classified_sf$recreation)] <- "FALSE" 

sas_tes_activities_classified_sf$recreation<-as.logical(sas_tes_activities_classified_sf$recreation)

sas_tes_activities_classified_sf$oev <- if_else(is.na(sas_tes_activities_classified_sf$obj_oev== TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$gebaeude <- if_else(is.na(sas_tes_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf[,c(1:8,17:19)]

###Classification ----
sas_test_classification <- sas_tes_activities_classified_sf|> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                            if_else(oev == TRUE, "travel",
                            if_else(recreation == TRUE, "recreation", "travel"),NA)))


sas_test_classification <- sas_test_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sas_test_classification <-sas_test_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sas_test_classification, dsn="CAMA_data/ cama__results_saskia_test.gpkg")#export classification results

###Confusion matrix ----
sas_test_classification<-na.omit(sas_test_classification)
confus_sas_tes_cama <-conf_mat(data = sas_test_classification, truth = Attribute_factor, estimate = activity_factor)

autoplot(confus_sas_tes_cama, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

###Compute model accuracy ----
confusionMatrix(sas_test_classification$Attribute_factor, sas_test_classification$activity_factor)

##Workflow with Sarah's test- data ----
###Read activity data  ----
sar_tes_activities_classified_sf <-read_sf("CAMA_data/activities_sarah_classified.gpkg")

sar_tes_activities_classified_sf <- st_transform(sar_tes_activities_classified_sf, crs = 2056)

###Join with objects -----
sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, bodenbuf,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_boden= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, nutzungsbuf , join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, strassenbuf ,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_oev= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, gebaeude_clip ,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_geb= objektart)

sar_tes_activities_classified_sf = subset(sar_tes_activities_classified_sf, 
                                  select = -c(uuid.x...9,uuid.y...11, uuid.x...13,uuid.y...15,uuid ))

###Create presence/absence information for objects ----
sar_tes_activities_classified_sf$recreation_b <- if_else(is.na(sar_tes_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$recreation_n <- if_else(is.na(sar_tes_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$recreation_s <- if_else(is.na(sar_tes_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE", 
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sar_tes_activities_classified_sf$recreation[is.na(sar_tes_activities_classified_sf$recreation)] <- "FALSE" 

sar_tes_activities_classified_sf$recreation<-as.logical(sar_tes_activities_classified_sf$recreation)

sar_tes_activities_classified_sf$oev <- if_else(is.na(sar_tes_activities_classified_sf$obj_oev== TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$gebaeude <- if_else(is.na(sar_tes_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf[,c(1:8,17:19)]

###Classification ----
sar_test_classification <- sar_tes_activities_classified_sf |> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                    if_else(oev == TRUE, "travel",
                    if_else(recreation == TRUE, "recreation", "travel"),NA)))


sar_test_classification <- sar_test_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sar_test_classification <-sar_test_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sar_test_classification, dsn="CAMA_data/ cama_classification_results_sarah.gpkg")#export sar_test_classification results

###Confusion matrix ----
sar_test_classification<-na.omit(sar_test_classification)
confus_sar_tes_cama <-conf_mat(data = sar_test_classification, truth = Attribute_factor, estimate = activity_factor)

autoplot(confus_sar_tes_cama, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

###Compute model accuracy ----
confusionMatrix(sar_test_classification$Attribute_factor, sar_test_classification$activity_factor)
```

```{r CART classification, results='hide', fig.keep='none'}
#| code-summary: CART based classification

##Workflow based on Saskia's training- data ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects ----
activities_attributes <-read_csv("test_activities_with_attributes_new.csv")
activities_attributes$ts_POSIXct <-as.POSIXct(activities_attributes$ts_POSIXct)

activities_attributes_sf <-st_as_sf(activities_attributes,
                                    coords = c("lon","lat"), crs = 4326 ,
                                    remove = FALSE) 

activities_attributes_sf <- st_transform(activities_attributes_sf, crs = 2056)

activities_attributes_sf  <-activities_attributes_sf [,c(1:26,30:32)]
#remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sas_train<-st_join(activities_attributes_sf,sas_tra_classification,  largest=TRUE)#largest = TRUE to remove duplicates

activities_sas_train$Attribute_factor.x <-as.factor(activities_sas_train$Attribute_factor.x)

###Create tree ----
set.seed(6832)
model_sas_train <-rpart(Attribute_factor.x~ speedMean+stepMean+acceleration+ recreation + oev + gebaeude, data=activities_sas_train, method= "class")

### Visualize tree ----
plot(model_sas_train)
text(model_sas_train, cex=0.8,use.n = TRUE, xpd = TRUE)

###See whether tree needs to be pruned ----
model_sas_train$cptable
#plotcp(model_sas_train)#no pruning needed

###Make prediction on original data for confusion matrix----
pred_model_sas_train<- predict(model_sas_train, type="class")
activities_sas_train$pred<-pred_model_sas_train

### Confusion matrix for training- data ---
confus_cart_train <-conf_mat(data =activities_sas_train, truth = Attribute_factor.x,estimate = pred)

autoplot(confus_cart_train, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

### Compute model accuracy rate on Sakia's training data----
confusionMatrix(activities_sas_train$Attribute_factor.x, activities_sas_train$pred)

##Make predictions on the test data from Saskia ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects ----

activities_attributes_sas_test <-read_csv("activities_saskia_with_attributes_classified_new.csv")
activities_attributes_sas_test$ts_POSIXct <-as.POSIXct(activities_attributes_sas_test$ts_POSIXct)

activities_attributes_sf_sas_test <-st_as_sf(activities_attributes_sas_test,
                                  coords = c("lon","lat"), crs = 4326 ,
                                  remove = FALSE)

activities_attributes_sf_sas_test  <- st_transform(activities_attributes_sf_sas_test , crs = 2056)

activities_attributes_sf_sas_test <-activities_attributes_sf_sas_test[,c(1:27,31:33)]##remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sas_test<-st_join(activities_attributes_sf_sas_test ,sas_test_classification, largest=TRUE)#largest = TRUE to remove duplicates

activities_sas_test$Attribute_factor.x <-as.factor(activities_sas_test$Attribute_factor.x)

pred_model_sas_test<- model_sas_train |>  
  predict(activities_sas_test, type = "class")

activities_sas_test$pred <-pred_model_sas_test

### Confusion matrix for Sakia's test- data  ---
confus_cart_test_sas <-conf_mat(data = activities_sas_test,
                           truth = Attribute_factor.x, estimate = pred)

autoplot(confus_cart_test_sas, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

### Compute model accuracy rate on Sakia's test data----
confusionMatrix(activities_sas_test$Attribute_factor.x, activities_sas_test$pred)

##Make predictions on the test data from Sarah ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects

activities_attributes_sarah <-read_csv("activities_sarah_with_attributes_classified_new.csv")
activities_attributes_sarah$ts_POSIXct <-as.POSIXct(activities_attributes_sarah$ts_POSIXct)

activities_attributes_sarah_sf <-st_as_sf(activities_attributes_sarah,
                                          coords = c("lon","lat"),
                                          crs = 4326 , remove = FALSE) 

activities_attributes_sarah_sf <- st_transform(activities_attributes_sarah_sf,
                                               crs = 2056)

activities_attributes_sarah_sf <-activities_attributes_sarah_sf[,c(1:27,31:33)]##remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sar_test<-st_join(activities_attributes_sarah_sf,sar_test_classification)#no largest required as the activity-data does not create duplicates

activities_sar_test$Attribute_factor.x <-as.factor(activities_sar_test$Attribute_factor.x)

pred_model_sar_test<- model_sas_train |>  
  predict(activities_sar_test, type = "class")

activities_sar_test$pred <-pred_model_sar_test

### Confusion matrix for Sarah's test- data  ---
confus_cart_sar_test <-conf_mat(data = activities_sar_test, 
                           truth = Attribute_factor.x, estimate = pred)

autoplot(confus_cart_sar_test, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

### Compute model accuracy rate on Sarah's test data----
confusionMatrix(activities_sar_test$Attribute_factor.x, activities_sar_test$pred)

```

```{r workflows}
#| code-summary: Visualisation of workflows
#rankdir = LR, label = '\n\n',labelloc = t

#Overall workflow ----
overall <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled,  fillcolor = white]
datatrain[label='training data Saskia', shape = folder, fillcolor = beige]
datatestsas[label='test data Saskia', shape = folder, fillcolor = beige]
datatestsar[label='test data Sarah', shape = folder, fillcolor = beige]
classify[label='classify manually \\n into actual walking patterns']
attribute[label='workflow attribute \\n based classification', fillcolor = aliceblue]
derattributes[label='derive attributes']
threshattributes[label='set thresholds based on  \\n summaries of trajectories \\n from training data']
classifyattributes[label='classify based on thresholds']
validate[label='validate with validation workflow']
camawork[label='workflow CAMA \\n based classification', fillcolor = aliceblue]
tlm[label='choose swissTLM3D layers, \\n which represent certain attributes']
buf[label='create buffers']
intersect[label='check for presence in buffers \\n position in buffers']
camaclass[label='classify based on \\n  presence in buffers']
cartwork [label ='workflow CART \\n based classification', fillcolor = aliceblue]
join[label='join data from \\n previous workflows \\n from the training data']
cart [label='create CART']

{datatrain datatestsas datatestsar} ->classify
classify -> {attribute camawork}
attribute -> derattributes
derattributes -> threshattributes
threshattributes -> classifyattributes
classifyattributes -> validate
camawork -> tlm
tlm -> buf
buf -> intersect
intersect -> camaclass
camaclass -> validate
{derattributes intersect} -> cartwork
cartwork -> join
join -> cart
cart -> validate
}") 

overall |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("overall.png")


# Validation workflow ----
validation_workflow <- grViz("digraph{
graph [layout = dot]
node [shape = rectangle, style = filled, rankdir = LR, fillcolor = white]
valwork[label='validation workflow', fillcolor = aliceblue]
valtrain[label='apply validation steps \\n on results from \\n training data']
applyattr[label='use thresholds \\n from attribute based classification \\n on test data']
applycama[label='use classification rules \\n from CAMA based classification \\n on test data']
applycart[label='apply CART tree to test data']
conf[label='create confusion matix']
acc[label='derive accuracy of classification']
valwork -> {valtrain applyattr applycama applycart}
{valtrain applyattr applycama applycart} -> {conf acc}
}")

validation_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("validation.png")

#Attribute based ----
attribute_based_workflow <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled, fillcolor = white]
travel[label='mean speed 1.7m/s - 4m/s \\n mean step 1.7m - 4m \\n mean acceleration < 0.003m^2']
travelyes[label='Yes']
traveltra[label='Is travel']
travelno[label='No']
recr[label='mean speed 1.1m/s - 1.7m/s \\n mean step 1.2m - 1.7m \\n mean acceleration <0.003m^2']
recryes[label='Yes']
recrtra[label='Is recreation']
recrno[label='No']
shop[label='mean speed > 4m/s or < 1.1m/s \\n mean step > 4m or < 1.2 \\n mean acceleration >0.001m^2']
shopyes[label='Yes']
shopyesshop[label='Is shopping']
shopno[label='No ']
shoptra[label='Is NA']

travel->{travelyes travelno}
travelyes->traveltra
travelno ->recr
recr -> {recryes recrno}
recryes -> recrtra
recrno -> shop
shop ->{shopyes shopno}
shopyes ->shopyesshop
shopno ->shoptra
}")

attribute_based_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("abw.png")

#CAMA based
cama_workflow <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled, fillcolor = white]

build[label='Is it in a building?']
buildyes[label='Yes']
buildshop[label='Is shopping']
buildno[label='No']
pubtrans[label='Is it in a 50m radius of a public transport station?']
pubtranspyes[label='Yes']
pubtransptra[label='Is travel']
pubtranspno[label='No']
recr[label=' Is it in a range of 10m of a recreational area?']
recryes[label='Yes']
recryesrec[label='Is recreation']
recrno[label='No ']
recrtra[label='Is travel']

build->{buildyes buildno}
buildyes->buildshop
buildno ->pubtrans
pubtrans -> {pubtranspyes pubtranspno}
pubtranspyes -> pubtransptra
pubtranspno -> recr
recr ->{recryes recrno}
recryes ->recryesrec
recrno ->recrtra
}")

cama_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("cama.png")
```


## Abstract

The main question this paper attempts to answer is whether it is possible to differentiate between the walking pattern from recreation, travel and shopping. Data is collected with the Strava App. Three approaches are developed, tested and evaluated. The first approach is a classification based on the attributes speed, step length and acceleration. The second approach is a classification based on the surroundings. The third approach is the development of a CART-tree that combines the attributes generated in the first approach and the context information from the second approach. We can conclude that the attribute-based classification is not able to cleanly separate travel and recreation. For shopping, there was no clear pattern to be found, as the GPS signal is most often lost in the building.

## Introduction

The aim of this paper is to test and compare different approaches to detect walking patterns. The different patterns that should be able to be distinguished are walking for recreational purposes, walking for commuting and shopping. The underlying idea is that the pattern of these walking types differs in their attributes and the environment that there are based in. It was already shown that a differentiation between transit and walking based on speed is possible [@kim_new_2012]. Also, outdoor walking activities are generally longer, more continuous and faster with the walking duration being the most important attribute for classification [@baroudi_classification_2024].\
Based on walking data tracked with the Strava app, we want to answer the following research questions:\
- How well it possible to derive the type of activity from movement data considering the attributes sinuosity, speed, sinuosity and visited locations?\
- Which type of analytical approach performs best in differentiating between the different activities?\
- How does the performance of the analytical concepts differ when applied on trajectories recorded by different people?\
The three approaches used in this study are classification based on attributes of the trajectory, CAMA-analysis and classification based on CART tree. The approaches are described in detail in the chapter material and methods. The following figure show the workflow for the three classification methods. \
![workflow](overall.png)

## Material and Methods

### Data

Data was collected with the Strava App from March to June 2024. The training data for the development of the algorithms consists of 12 trajectories recorded by Saskia in March and April 2024. There are 13'636 attributed points. Additional data is collected by Saskia and Sarah in June 2024 to enable testing of the developed algorithms. Additional three trajectories recorded from Saskia in 2023 are used to test the algorithms. The algorithms are tested separately on the data of both of us. This allows us to draw a conclusion whether the algorithms are able to differentiate walking patterns from different people. The test data from Saskia consist of 6 trajectories and a total of 8'559 attributed points. The test data from Sarah consists of 12 trajectories and a total of 17'648 attributed points. 

### Preparation

The data exported from Strava has the format gpx. The single activities are read and written into a data frame. IDs and descriptions for recognition are assigned to the single activities. The activities are then combined to one data frame and turned into a spatial object, which is then exported for manual classification. In QGIS, every point is given either the attribute shopping, recreation or commuting. The points are selected by location using an Open Street Map basemap. The activities are then again imported, turned into a spatial object and reprojected to LV95.


### Attribute-based classification

For the attribute-based classification, the time-lag and distance between single locations are being calculated. These calculations are checked for outlines. As the time lag between two consecutive locations is mostly 1 s, all intervals bigger than 5 s are removed, as these are most likely a mistake. The same is true for the distance between two consecutive locations. All step length more than 5 m are removed. As attributes also depend on the spatial scale, the mean step is calculated using the two previous and two next distances. The speed is deducted from this mean step. The segmentation is done as described in @laube_how_2011. The threshold for moving is set to 1.5 m. This is based on the assumption that this equals around 3 - 4 steps, which can be interpreted as moving. Additionally, acceleration was calculated based on the calculated speed and time lag. After the calculation of the attributes per trajectories, a summary is done for each trajectory to find differences between activity types. Also, a summary over all activities is done to find differences between activity types. This information was used to set the thresholds. Based on the established differences, a classification is done. The classification is verified using a confusion matrix and its statistic output. The thresholds for classification are adapted until the best possible classification is reached. The following figure shows the workflow for the attribute-based classification. \
![workflow](abw.png)

### CAMA classification
text \

![workflow](cama.png)

### Validation

text \

![workflow](validation.png)

### CART classification
text


## Results

### Attribute based classification

#### Training data

Despite extensive testing and adapting of the thresholds, it was not possible to correctly classify all activities. Based on the attributes speed, step length and acceleration, activities were hard to distinguish. 49 out of 12'886 points could not be classified and were removed before computing the confusion matrix. 7'526 points were classified correctly, which leads to an overall accuracy of 59 %. Sensitivity is highest for the class travel (0.61), specificity is highest for the class shopping (0.98). Detection rate is highest for travel (0.54). Travel and recreation were most difficult to differentiate. 

```{r confusion matrix, message=FALSE, results='hide', fig.cap="Confusion matrix for the training data. The overall accuracy is 59 %. Despite adapting thresholds, travel and recreation could not be clearly differentiated."}
#| code-summary: confusion matrix training data
# plot confusion matrix for test data
autoplot(confus_training, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")
```

#### Activity data Saskia

30 out of 6826 points could not be classified and were removed before computing the confusion matrix. 2270 points were classified correctly, leading to an overall accuracy of the model for the activity data of Saskia of 33 %. Again, sensitivity is highest for travel (0.34), as well as the detection rate (0.33).

```{r confusion matrix Saskias data, message=FALSE, results='hide', fig.cap="Confusion matrix for Saskias data. The accuracy is 33 %. Most misclassifications occur with travel and recreation."}
#| code-summary: Confusion matrix for Saskias data
# plot confusion matrix
autoplot(confus_saskia, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")
```

#### Activity data Sarah

51 out of 17'061 points could not be classified and were removed before computing the confusion matrix. For Sarahs activities, 3209 points were classified correctly. The accuracy of the model is 19 %. The sensitivity for recreation is highest (0.71), the detection rate is highest for travel (0.16).

```{r confusion matrix Sarahs data, message=FALSE, results='hide', fig.cap="Confusion matrix for Sarahs data. The accuracy is 19 %. 95 % of the points are classified as travel."}
#| code-summary: Confusion matrix Sarahs data
# plot confusion matrix
autoplot(confus_sarah, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```


## Discussion

## Appendix

### Wordcount

<!-- after installing the wordcountadding, remove the line "#| eval: false" -->

```{r}
wordcountaddin::word_count("index.qmd")
```
