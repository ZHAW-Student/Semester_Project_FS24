---
title: Differentiation of walking patterns
subtitle: An attempt to differentiate walking patterns based on attributes and context information \Project Work for the module Patterns and Trends FS24
author: Saskia Gianola and Sarah Wirth
output: bookdown::html_document2
format:
  html:
    code-fold: true
execute:
  warning: false
  message: false
lang: en
bibliography: bibliography.bib
---
```{r preprocessing_attribute, results='hide', fig.keep='none'}
#| code-summary: Preprocessing for Attribute based classification
# used libraries #### 
library("XML")
library("leaflet")
library("sf")
library("tmap")
library("ggplot2")
library("tidyverse")
library("readr")
library("trajr")
library("yardstick")
library("caret")
library("DiagrammeR")
library("png")
library("DiagrammeRsvg")
library("magrittr")
library("rsvg")
library("readr")
library("dplyr")
library("rpart")
library("stringr")

# Preprocessing ####
# read training activities
training_files <- list.files("activities/.", pattern = "*.gpx")

for (i in 1:length(training_files)){
  filename <- paste0("act", i)
  wd <- paste0("activities/", training_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# function to parse activities and write into data frame
built_df <- function(activity) {
  # get coordinates
  coords <- xpathSApply(doc = activity, path = "//trkpt", fun = xmlAttrs)
  # get elevation
  elevation <- xpathSApply(doc = activity, path = "//trkpt/ele", fun = xmlValue)
  # get time
  time <- xpathApply(doc = activity, path = "//trkpt/time", fun = xmlValue)
  data.frame(
    lat = as.numeric(coords["lat", ]),
    lon = as.numeric(coords["lon", ]),
    ts_POSIXct = ymd_hms(time, tz = "UTC"),
    elevation = as.numeric(elevation)
  )
}
# convert files to dataframe
act1_df <- built_df(act1)
act2_df <- built_df(act2)
act3_df <- built_df(act3)
act4_df <- built_df(act4)
act5_df <- built_df(act5)
act6_df <- built_df(act6)
act7_df <- built_df(act7)
act8_df <- built_df(act8)
act9_df <- built_df(act9)
act10_df <- built_df(act10)
act11_df <- built_df(act11)
act12_df <- built_df(act12)

# assign ID and description to single files
act1_df$ID <- "test_1"
act1_df$ID_text <- "test_Waedenswil_Reidbach_Zentrum"
act2_df$ID <- "test_2"
act2_df$ID_text <- "test_Waedenswil_Coop_Bahnhof"
act3_df$ID <- "test_3"
act3_df$ID_text <- "test_Waedenswil_Schloss_Mensa"
act4_df$ID <- "test_4"
act4_df$ID_text <- "test_Waedenswil_Gruental_Bahnhof1"
act5_df$ID <- "test_5"
act5_df$ID_text <- "test_Waedenswil_Gruental_Bahnhof2"
act6_df$ID <- "test_6"
act6_df$ID_text <- "test_Rueti_Home_Bahnhof_Coop_Home"
act7_df$ID <- "test_7"
act7_df$ID_text <- "test_test_Rapperswil_See_Bahnhof"
act8_df$ID <- "test_8"
act8_df$ID_text <- "test_Rueti_Home_Coop_Home"
act9_df$ID <- "test_9"
act9_df$ID_text <- "test_Neuhausen_Bahnhof_Rhein"
act10_df$ID <- "test_10"
act10_df$ID_text <- "test_Neuhausen_Rhein_Bahnhof"
act11_df$ID <- "test_11"
act11_df$ID_text <- "test_S-chanf"
act12_df$ID <- "test_12"
act12_df$ID_text <- "test_Regensdorf_Buero_Coop"

# function to convert data frame into sf object
df_to_sf <- function(df){
  st_as_sf(df, coords = c("lon", "lat"), crs = 4326 , remove = FALSE)
}

# combine all activitites into one data frame
training_activities_df <- rbind(act1_df, act2_df, act3_df, act4_df, act5_df, act6_df, act7_df, act8_df, act9_df, act10_df, act11_df, act12_df)

# turn data frame into sf object
training_activities_sf <- df_to_sf(training_activities_df)

# export sf object for setting attributes in GIS
# export_test_activities <- st_write(test_activities_sf, "test_activities.shp")

# read Saskias activities
saskia_files <- list.files("activities/activities_saskia.", pattern = "*.gpx")

for (i in 1:length(saskia_files)){
  filename <- paste0("saskia_", i)
  wd <- paste0("activities/activities_saskia/", saskia_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# convert files to dataframe
saskia_1_df <- built_df(saskia_1)
saskia_2_df <- built_df(saskia_2)
saskia_3_df <- built_df(saskia_3)
saskia_4_df <- built_df(saskia_4)
saskia_5_df <- built_df(saskia_5)
saskia_6_df <- built_df(saskia_6)

# assign ID to single files
saskia_1_df$ID <- "saskia__1"
saskia_1_df$ID_text <- "saskia_Pfaeffikon_Seedamm1"
saskia_2_df$ID <- "saskia__2"
saskia_2_df$ID_text <- "saskia_Pfaeffikon_Seedamm2"
saskia_3_df$ID <- "saskia__3"
saskia_3_df$ID_text <- "saskia_Regensdorf_Buero_Bahnhof"
saskia_4_df$ID <- "saskia__4"
saskia_4_df$ID_text <- "saskia_Rueti_Zentrum_Home"
saskia_5_df$ID <- "saskia__5"
saskia_5_df$ID_text <- "saskia_Rueti_Home_Bahnhof_Coop_Home"
saskia_6_df$ID <- "saskia__6"
saskia_6_df$ID_text <- "saskia_Waedenswil_Gruental_Bhf"

# combine all activitites to one data frame
activities_saskia_df <- rbind(saskia_1_df, saskia_2_df, saskia_3_df, saskia_4_df, saskia_5_df, saskia_6_df)

# turn data frame into sf object
activities_saskia_sf <- df_to_sf(activities_saskia_df)

# export sf object for setting attributes in GIS
# export_activities_saskia <- st_write(activities_saskia_sf, "activities_saskia.csv")

# read Sarahs activities
sarah_files <- list.files("activities/activities_sarah", pattern = "*.gpx")

for (i in 1:length(sarah_files)){
  filename <- paste0("sarah_", i)
  wd <- paste0("activities/activities_sarah/", sarah_files[i])
  assign(filename, htmlTreeParse(file = wd, useInternalNodes = TRUE))
}

# convert files to dataframe
sarah_1_df <- built_df(sarah_1)
sarah_2_df <- built_df(sarah_2)
sarah_3_df <- built_df(sarah_3)
sarah_4_df <- built_df(sarah_4)
sarah_5_df <- built_df(sarah_5)
sarah_6_df <- built_df(sarah_6)
sarah_7_df <- built_df(sarah_7)
sarah_8_df <- built_df(sarah_8)
sarah_9_df <- built_df(sarah_9)
sarah_10_df <- built_df(sarah_10)
sarah_11_df <- built_df(sarah_11)
sarah_12_df <- built_df(sarah_12)

# assign ID and description to single files
sarah_1_df$ID <- "sarah__1"
sarah_1_df$ID_text <- "sarah_Denner_Einkauf"
sarah_2_df$ID <- "sarah__2"
sarah_2_df$ID_text <- "sarah_Klafi_Feld"
sarah_3_df$ID <- "sarah__3"
sarah_3_df$ID_text <- "sarah_Klafi_von_Zug_Heim"
sarah_4_df$ID <- "sarah__4"
sarah_4_df$ID_text <- "sarah_Migros_groesserer_Einkauf"
sarah_5_df$ID <- "sarah__5"
sarah_5_df$ID_text <- "sarah_Migros_kleiner_Einkauf"
sarah_6_df$ID <- "sarah__6"
sarah_6_df$ID_text <- "sarah_Rosengarten_und_Shopping_Winti"
sarah_7_df$ID <- "sarah__7"
sarah_7_df$ID_text <- "sarah_Spaziergang_am_Morgen_Bub"
sarah_8_df$ID <- "sarah__8"
sarah_8_df$ID_text <- "sarah_Spaziergang_Klafi_kleine_Runde"
sarah_9_df$ID <- "sarah__9"
sarah_9_df$ID_text <- "sarah_WB_Heim"
sarah_10_df$ID <- "sarah__10"
sarah_10_df$ID_text <- "sarah_Zu_Bus_Klafi"
sarah_11_df$ID <- "sarah__11"
sarah_11_df$ID_text <- "sarah_Zum_Bus_Bub"
sarah_12_df$ID <- "sarah__12"
sarah_12_df$ID_text <- "sarah_Zum_Zug_Liestal"

# combine all activities to one data frame
activities_sarah_df <- rbind(sarah_1_df, sarah_2_df, sarah_3_df, sarah_4_df, sarah_5_df, sarah_6_df,sarah_7_df,sarah_8_df,sarah_9_df, sarah_10_df, sarah_11_df,sarah_12_df)

# turn data frame into sf object
activities_sarah_sf <- df_to_sf(activities_sarah_df)

#create new row for activity- classification and classify trajectories with only one activity
activities_sarah_sf<-activities_sarah_sf |> 
  mutate(Attribut = case_when(ID_text == "sarah_Denner_Einkauf" ~ "recreation", 
                              ID_text == "sarah_Spaziergang_am_Morgen_Bub"~ "recreation",
                              ID_text == "sarah_Spaziergang_Klafi_kleine_Runde"  ~ "recreation", 
                              ID_text == "sarah_Klafi_von_Zug_Heim"  ~ "travel",
                              ID_text == "sarah_WB_Heim"~ "travel",
                              ID_text == "sarah_Zu_Bus_Klafi"~ "travel",
                              ID_text == "sarah_Zum_Bus_Bub"~ "travel",
                              ID_text == "sarah_Zum_Zug_Liestal"~ "travel",
                              ID_text == "sarah_Klafi_Feld"~ "recreation" ,
                              ID_text == "sarah_Migros_groesserer_Einkauf"~ "recreation",
                              ID_text == "sarah_Migros_kleiner_Einkauf"~ "recreation" ,
                              ID_text == "sarah_Rosengarten_und_Shopping_Winti"~ "recreation"  ))

# export sf object for setting attributes in GIS
# export_activities_sarah <- st_write(activities_sarah_sf, "activities_sarah.csv")
## Visualisation of workflows ####
#rankdir = LR, label = '\n\n',labelloc = t

#Overall workflow ----
overall <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled,  fillcolor = white]
datatrain[label='training data Saskia', shape = folder, fillcolor = beige]
datatestsas[label='test data Saskia', shape = folder, fillcolor = beige]
datatestsar[label='test data Sarah', shape = folder, fillcolor = beige]
classify[label='classify manually \\n into actual walking patterns']
attribute[label='workflow attribute \\n based classification', fillcolor = aliceblue]
derattributes[label='derive attributes']
threshattributes[label='set thresholds based on  \\n summaries of trajectories \\n from training data']
classifyattributes[label='classify based on thresholds']
validate[label='validate with validation workflow']
camawork[label='workflow CAMA \\n based classification', fillcolor = aliceblue]
tlm[label='choose swissTLM3D layers, \\n which represent certain attributes']
buf[label='create buffers']
intersect[label='check for presence in buffers \\n position in buffers']
camaclass[label='classify based on \\n  presence in buffers']
cartwork [label ='workflow CART \\n based classification', fillcolor = aliceblue]
join[label='join data from \\n previous workflows \\n from the training data']
cart [label='create CART']

{datatrain datatestsas datatestsar} ->classify
classify -> {attribute camawork}
attribute -> derattributes
derattributes -> threshattributes
threshattributes -> classifyattributes
classifyattributes -> validate
camawork -> tlm
tlm -> buf
buf -> intersect
intersect -> camaclass
camaclass -> validate
{derattributes intersect} -> cartwork
cartwork -> join
join -> cart
cart -> validate
}") 

overall |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("overall.png")


# Validation workflow ----
validation_workflow <- grViz("digraph{
graph [layout = dot]
node [shape = rectangle, style = filled, rankdir = LR, fillcolor = white]
valwork[label='validation workflow', fillcolor = aliceblue]
valtrain[label='apply validation steps \\n on results from \\n training data']
applyattr[label='use thresholds \\n from attribute based classification \\n on test data']
applycama[label='use classification rules \\n from CAMA based classification \\n on test data']
applycart[label='apply CART tree to test data']
conf[label='create confusion matix']
acc[label='derive accuracy of classification']
valwork -> {valtrain applyattr applycama applycart}
{valtrain applyattr applycama applycart} -> {conf acc}
}")

validation_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("validation.png")

#Attribute based ----
attribute_based_workflow <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled, fillcolor = white]
travel[label='mean speed 1.7m/s - 4m/s \\n mean step 1.7m - 4m \\n mean acceleration < 0.003m^2']
travelyes[label='Yes']
traveltra[label='Is travel']
travelno[label='No']
recr[label='mean speed 1.1m/s - 1.7m/s \\n mean step 1.2m - 1.7m \\n mean acceleration <0.003m^2']
recryes[label='Yes']
recrtra[label='Is recreation']
recrno[label='No']
shop[label='mean speed > 4m/s or < 1.1m/s \\n mean step > 4m or < 1.2 \\n mean acceleration >0.001m^2']
shopyes[label='Yes']
shopyesshop[label='Is shopping']
shopno[label='No ']
shoptra[label='Is NA']

travel->{travelyes travelno}
travelyes->traveltra
travelno ->recr
recr -> {recryes recrno}
recryes -> recrtra
recrno -> shop
shop ->{shopyes shopno}
shopyes ->shopyesshop
shopno ->shoptra
}")

attribute_based_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("abw.png")

#CAMA based
cama_workflow <- grViz("digraph{
graph [layout = dot, rankdir = LR]
node [shape = rectangle, style = filled, fillcolor = white]

build[label='Is it in a building?']
buildyes[label='Yes']
buildshop[label='Is shopping']
buildno[label='No']
pubtrans[label='Is it in a 50m radius of a public transport station?']
pubtranspyes[label='Yes']
pubtransptra[label='Is travel']
pubtranspno[label='No']
recr[label=' Is it in a range of 10m of a recreational area?']
recryes[label='Yes']
recryesrec[label='Is recreation']
recrno[label='No ']
recrtra[label='Is travel']

build->{buildyes buildno}
buildyes->buildshop
buildno ->pubtrans
pubtrans -> {pubtranspyes pubtranspno}
pubtranspyes -> pubtransptra
pubtranspno -> recr
recr ->{recryes recrno}
recryes ->recryesrec
recrno ->recrtra
}")

cama_workflow |> 
  export_svg() |> 
  charToRaw() |> 
  rsvg_png("cama.png")
```

```{r preprocessing_CAMA, results='hide', fig.keep='none', eval=FALSE}
#| code-summary: Preprocessing CAMA based classification
# We decide not to let this chunk run as it relies on the swissTLM3 dataset and therefore takes quite long to run. All outputs generated from those steps are included in the code chunk for the CAMA based classification

##Prepare clip data----
gemeindegrenzen <- read_sf("swissBOUNDARIES3D_1_5_LV95_LN02.gpkg","tlm_hoheitsgebiet")

gemeindeselection <-gemeindegrenzen |>
                  filter(name %in% c("Andelfingen","Bubendorf","Freienbach" 
                                     ,"Kleinandelfingen","Liestal" ,"Neuhausen am
                                     Rheinfall","Rapperswil-Jona","Regensdorf","
                                     Rüti","St. Moritz","S-chanf","Wädenswil"
                                     ,"Winterthur","Zuoz"))

##Reading, clipping and exporting clipped layers----
st_layers("SWISSTLM3D_2024_LV95_LN02.gpkg")#see all contents of geopackage

################### Warning! This does take some time.  

gebaeude <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_bauten_gebaeude_footprint")
gebaeude_selection<-st_intersection(gebaeude,gemeindeselection)
rm(gebaeude)
st_write(gebaeude_selection, dsn="CAMA_data/gebaeude_selection.gpkg")

bodenbedeckung <- read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_bb_bodenbedeckung")
bodenbedeckung_selection<-st_intersection(bodenbedeckung,gemeindeselection)
rm(bodenbedeckung)
st_write(bodenbedeckung_selection, dsn="CAMA_data/bodenbedeckung_selection.gpkg")

nutzungsareal <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_areale_nutzungsareal")
nutzungsareal_selection<-st_intersection(nutzungsareal,gemeindeselection)
rm(nutzungsareal)
st_write(nutzungsareal_selection, dsn="CAMA_data/nutzungsareal_selection.gpkg")

strassen <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_strassen_strasse")
strassen_selection<-st_intersection(strassen,gemeindeselection)
rm(strassen)
st_write(strassen_selection, dsn="CAMA_data/strassen_selection.gpkg")

oev <-read_sf("SWISSTLM3D_2024_LV95_LN02.gpkg","tlm_oev_haltestelle")
oev_selection<-st_intersection(oev,gemeindeselection)
rm(oev)
st_write(oev_selection, dsn="CAMA_data/oev_selection.gpkg")

```

```{r attribute-based classification, results='hide', fig.keep='none'}
#| code-summary: Attribute based classification

# attribute-based classification for training data ####
# import csv with attributes and convert to sf
training_activities_attributed <- read_delim("test_activities_attributiert.csv", ",")
training_activities_attributed_sf <- df_to_sf(training_activities_attributed)

# change crs of sf and specify attributes
training_activities_attributed_sf <- st_transform(training_activities_attributed_sf, crs = 2056)

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# Calculate attributes 
## calculate time lag  
difftime_secs <- function(later, now){
  as.numeric(difftime(later, now, units = "secs"))
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

## calculate distance between locations
distance_by_element <- function(later, now){
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

## Check plausibility of calculated parameters
plot(training_activities_attributed_sf$timelag_sec)
plot(training_activities_attributed_sf$steplenght)
boxplot(training_activities_attributed_sf$timelag_sec)
boxplot(training_activities_attributed_sf$steplenght)
summary(training_activities_attributed_sf$timelag_sec)
summary(training_activities_attributed_sf$steplenght)

# based on this check, remove all timelag > 5 and steplenght > 5
outliers_training_timelag <- filter(training_activities_attributed_sf, timelag_sec >= 5)
training_activities_attributed_sf <- training_activities_attributed_sf[which(training_activities_attributed_sf$timelag_sec <= 5),]
outliers_training_steplenght <- filter(training_activities_attributed_sf, steplenght >= 5)
training_activities_attributed_sf <- training_activities_attributed_sf[which(training_activities_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(training_activities_attributed_sf$timelag_sec)
plot(training_activities_attributed_sf$steplenght)
boxplot(training_activities_attributed_sf$timelag_sec)
boxplot(training_activities_attributed_sf$steplenght)

# Segmentation
# Specify a temporal window for mean step
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )

# calculate mean step
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) |>
  ungroup()

# calculate mean speed
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# Explore mean step to define threshold 
hist(training_activities_attributed_sf$stepMean)
boxplot(training_activities_attributed_sf$stepMean)
summary(training_activities_attributed_sf$stepMean)

# apply threshold
training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
rle_id <- function(vec) {
  x <- rle(vec)$lengths
  as.factor(rep(seq_along(x), times = x))
}

training_activities_attributed_sf <- training_activities_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static))|> 
  ungroup()

# extract single trajectories
traj1 <- filter(training_activities_attributed_sf, ID == "test_1")
traj2 <- filter(training_activities_attributed_sf, ID == "test_2")
traj3 <- filter(training_activities_attributed_sf, ID == "test_3")
traj4 <- filter(training_activities_attributed_sf, ID == "test_4")
traj5 <- filter(training_activities_attributed_sf, ID == "test_5")
traj6 <- filter(training_activities_attributed_sf, ID == "test_6")
traj7 <- filter(training_activities_attributed_sf, ID == "test_7")
traj8 <- filter(training_activities_attributed_sf, ID == "test_8")
traj9 <- filter(training_activities_attributed_sf, ID == "test_9")
traj10 <- filter(training_activities_attributed_sf, ID == "test_10")
traj11 <- filter(training_activities_attributed_sf, ID == "test_11")
traj12 <- filter(training_activities_attributed_sf, ID == "test_12")


# Summarize by true activity
summary_training <- training_activities_attributed_sf |> 
  group_by(ID) |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))

# summarize per trajectory
stops_traj1 <- traj1 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj2 <- traj2 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj3 <- traj3 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj4 <- traj4 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj5 <- traj5 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj6 <- traj6 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj7 <- traj7 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj8 <- traj8 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj9 <- traj9 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj10 <- traj10 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj11 <- traj11 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))
stops_traj12 <- traj12 |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE),
            not_stops = sum(!static, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE))

# function to calculate acceleration
acceleration <- function(s1, s2, t){
  as.numeric((s2-s1)/(t))
}

# calculate acceleration 
training_activities_attributed_sf <- training_activities_attributed_sf |> 
mutate(acceleration = 
    acceleration(lag(speedMean), speedMean, timelag_sec))

# summarize attributes per attribute factor
summary_training <- training_activities_attributed_sf |> 
  group_by(ID) |> 
  group_by(Attribute_factor) |> 
  summarize(stops = sum(static, na.rm = TRUE), 
            not_stops = sum(!static, na.rm = TRUE),
            stop_ratio = not_stops/stops,
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_lenght = length(segment_id)
            ) |> 
  ungroup()

# create new id to group by for classification
training_activities_for_classification <- training_activities_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))
  

# summarize with new id
training_classification <- training_activities_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
            ) |> 
  ungroup()

# apply thresholds for attributes
training_classification <- training_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                              ,  1 ,  0 ))

# create new attribute with activity as character based on applied thresholds
training_classification <- training_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))

# remove points where classification was not possible
training_classification <- na.omit(training_classification)

# define activity as factor
training_classification <- training_classification |> 
  mutate(activity_factor = as.factor(activity)) 
  
# join classified table with sf object
training_activities_classified <- st_join(training_activities_for_classification, training_classification, left = TRUE)

# calculate confusion matrix to test classification
confus_training <-conf_mat(data = training_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# Export csv for other approaches
# st_write(training_activities_classified, "test_activities_with_attributes_new.csv")

# get statistics for confusion matrix 
confusionMatrix(training_activities_classified$Attribute_factor, training_activities_classified$activity_factor)

# attribute-based classification Saskias data ####
# import csv with attributes and convert to sf
activities_saskia_attributed <- read_delim("activities_saskia_attributiert.csv", ",")
activities_saskia_attributed_sf <- df_to_sf(activities_saskia_attributed)

# change crs of sf
activities_saskia_attributed_sf <- st_transform(activities_saskia_attributed_sf, crs = 2056)

# set attributes
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# calculate time lag
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

# calculate distance between locations
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

# Check plausibility of calculated parameters
plot(activities_saskia_attributed_sf$timelag_sec)
plot(activities_saskia_attributed_sf$steplenght)
boxplot(activities_saskia_attributed_sf$timelag_sec)
boxplot(activities_saskia_attributed_sf$steplenght)
summary(activities_saskia_attributed_sf$timelag_sec)
summary(activities_saskia_attributed_sf$steplenght)

# based on this check, remove all timelag > 5 and steplenght > 5
outliers_timelag <- filter(activities_saskia_attributed_sf, timelag_sec >= 5)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf[which(activities_saskia_attributed_sf$timelag_sec <= 5),]
outliers_steplenght <- filter(activities_saskia_attributed_sf, steplenght >= 5)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf[which(activities_saskia_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(activities_saskia_attributed_sf$timelag_sec)
plot(activities_saskia_attributed_sf$steplenght)
boxplot(activities_saskia_attributed_sf$timelag_sec)
boxplot(activities_saskia_attributed_sf$steplenght)


# Segmentation
# Specify a temporal window for mean step
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )
# calculate mean step
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) 

# calculate mean speed
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# apply threshold (same as for test data)
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static)) |> 
  ungroup()

# calculate acceleration
activities_saskia_attributed_sf <- activities_saskia_attributed_sf |> 
  mutate(acceleration = 
           acceleration(lag(speedMean), speedMean, timelag_sec))

# create new id for classification
activities_saskia_for_classification <- activities_saskia_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))

# group by new id and summarize attributes
saskia_classification <- activities_saskia_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
  ) |> 
  ungroup()

# apply thresholds defined for test data
saskia_classification <- saskia_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                            ,  1 ,  0 ))

# create attribute with activity as text
saskia_classification <- saskia_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))
# remove points that could not be classified
saskia_classification <- na.omit(saskia_classification)

# set classified activity as factor
saskia_classification <- saskia_classification |> 
  mutate(activity_factor = as.factor(activity)) 

# join classified table with sf object
saskia_activities_classified <- st_join(activities_saskia_for_classification, saskia_classification, left = TRUE)

# Export csv for other approaches
#st_write(saskia_activities_classified, "activities_saskia_with_attributes_classified_new.csv")

# generate confusion matrix for data
confus_saskia <-conf_mat(data = saskia_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# get statistics of confusion matrix
confusionMatrix(saskia_activities_classified$Attribute_factor, saskia_activities_classified$activity_factor)

# attribute-based classification Sarahs data ####
# import csv with attributes and convert to sf
activities_sarah_attributed <- read_delim("activities_sarah_attributiert.csv", ",")
activities_sarah_attributed_sf <- df_to_sf(activities_sarah_attributed)

# change crs of sf
activities_sarah_attributed_sf <- st_transform(activities_sarah_attributed_sf, crs = 2056)

# set attributes
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  mutate(
    DateTime = as.POSIXct(ts_POSIXct),
    Attribute_factor = as.factor(Attribut)
  )

# calculate time lag
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    timelag_sec = difftime_secs(lead(DateTime,), DateTime)
  )

#calculate distance between locations
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    steplenght = distance_by_element(lag(geometry), geometry)
  )

# Check plausibility of calculated parameters
plot(activities_sarah_attributed_sf$timelag_sec)
plot(activities_sarah_attributed_sf$steplenght)
boxplot(activities_sarah_attributed_sf$timelag_sec)
boxplot(activities_sarah_attributed_sf$steplenght)
summary(activities_sarah_attributed_sf$timelag_sec)
summary(activities_sarah_attributed_sf$steplenght)

# based on this check, all timelag > 5 and steplenght > 5
outliers_timelag <- filter(activities_sarah_attributed_sf, timelag_sec >= 5)
activities_sarah_attributed_sf <- activities_sarah_attributed_sf[which(activities_sarah_attributed_sf$timelag_sec <= 5),]
outliers_steplenght <- filter(activities_sarah_attributed_sf, steplenght >= 5)
activities_sarah_attributed_sf <- activities_sarah_attributed_sf[which(activities_sarah_attributed_sf$steplenght <= 5),]

# plot again to make sure it's better
plot(activities_sarah_attributed_sf$timelag_sec)
plot(activities_sarah_attributed_sf$steplenght)
boxplot(activities_sarah_attributed_sf$timelag_sec)
boxplot(activities_sarah_attributed_sf$steplenght)

# Segmentation
# Specify a temporal window for mean step
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
    nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
    nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
    nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
  )
# calculate mean step
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  rowwise() |>
  mutate(
    stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
  ) 

# calculate mean speed
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(
    speedMean = stepMean/timelag_sec
  )

# apply threshold defined in test data
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(static = stepMean <  1.5)

# give segments an ID
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  group_by(ID) |>
  mutate(segment_id = rle_id(static)) |> 
  ungroup()

# calculate acceleration ####
activities_sarah_attributed_sf <- activities_sarah_attributed_sf |> 
  mutate(acceleration = 
           acceleration(lag(speedMean), speedMean, timelag_sec))

# create new id to group for classification
activities_sarah_for_classification <- activities_sarah_attributed_sf |> 
  mutate(combi_ID = paste(ID, segment_id, sep = "_"))

# group by newly set id
sarah_classification <- activities_sarah_for_classification |> 
  group_by(combi_ID) |> 
  summarize(mean_acceleration = mean(acceleration, na.rm = TRUE),
            mean_speed = mean(speedMean, na.rm =TRUE),
            mean_step = mean(stepMean, na.rm = TRUE),
            lenght = length(segment_id)
  ) |> 
  ungroup()

# apply defined thresholds
sarah_classification <- sarah_classification |> 
  mutate(travel = if_else(mean_speed > 1.7 & mean_speed < 4
                          & mean_step > 1.7 & mean_step < 4
                          & mean_acceleration < 0.003,  1 ,  0 ),
         recreation = if_else(travel %in% c( 0 ) 
                              & mean_speed > 1.1  & mean_speed < 1.7
                              & mean_step > 1.2 & mean_step < 1.7
                              & mean_acceleration < 0.003,  1 ,  0 ),
         shopping = if_else(travel %in% c( 0 ) 
                            & recreation %in% c( 0 )
                            & mean_speed > 4 |  mean_speed < 1.1
                            & mean_step > 4 | mean_step < 1.2
                            & mean_acceleration > 0.01
                            ,  1 ,  0 ))

# get attribute activity as text
sarah_classification <- sarah_classification |> 
  mutate(activity = if_else(shopping == 1, "shopping", 
                            if_else(recreation == 1, "recreation", "travel"),"NA"))

# remove points that could not be classified
sarah_classification <- na.omit(sarah_classification)

# set classified activity as factor
sarah_classification <- sarah_classification |> 
  mutate(activity_factor = as.factor(activity)) 

# join classification table with sf object
sarah_activities_classified <- st_join(activities_sarah_for_classification, sarah_classification, left = TRUE)

# Export csv for other approaches
#st_write(sarah_activities_classified, "activities_sarah_with_attributes_classified_new.csv")

# calcualte confusion matrix
confus_sarah <-conf_mat(data = sarah_activities_classified, truth = Attribute_factor, estimate = activity_factor)

# get statitics for confusion matrix
confusionMatrix(sarah_activities_classified$Attribute_factor, sarah_activities_classified$activity_factor)

```

```{r CAMA classification, results='hide', fig.keep='none'}
#| code-summary: CAMA based classification

##Re-read clipped data, remove not- needed columns and filter if required ----
gebaeude_clip<-read_sf("CAMA_data/gebaeude_selection.gpkg")
gebaeude_clip <- gebaeude_clip[,c(1,10,40)]

bodenbedeckung_clip<-read_sf("CAMA_data/bodenbedeckung_selection.gpkg")
bodenbedeckung_clip<-bodenbedeckung_clip[,c(1,10,37)]

nutzungsareal_clip<-read_sf("CAMA_data/nutzungsareal_selection.gpkg")
nutzungsareal_clip<-nutzungsareal_clip[,c(1,11,39)]

strassen_clip<-read_sf("CAMA_data/strassen_selection.gpkg")
#reduce roads to roads associated with recreational activities
strassen_recreational <-strassen_clip |> 
                      filter(objektart %in% c("Ausfahrt","Einfahrt","Zufahrt",
                                              "3m Strasse","1m Weg" ,"2m Weg",
                                              "1m Wegfragment","2m Wegfragment"))
rm(strassen_clip)
strassen_recreational<-strassen_recreational[,c(1,10,51)]

oev_clip<-read_sf("CAMA_data/oev_selection.gpkg")
oev_clip<-oev_clip[,c(1,10,40)]

##Create Buffers ----
bodenbuf <-st_buffer(bodenbedeckung_clip, dist=10)
rm(bodenbedeckung_clip)
nutzungsbuf <-st_buffer(nutzungsareal_clip, dist=10)
rm(nutzungsareal_clip)
strassenbuf <-st_buffer(strassen_recreational, dist=10)
rm(strassen_recreational)
oevbuf <-st_buffer(oev_clip, dist=50)
rm(oev_clip)

##Workflow with Saskia's training- data ----
###Read activity data ----
sas_tra_activities_classified_sf <-read_sf("CAMA_data/test_activities_attributiert.gpkg")

sas_tra_activities_classified_sf <- st_transform(sas_tra_activities_classified_sf, crs = 2056)

###Join with objects -----
sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, bodenbuf, join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_boden= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, nutzungsbuf ,  join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, strassenbuf, join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_pub_trans= objektart)

sas_tra_activities_classified_sf<-st_join(sas_tra_activities_classified_sf, gebaeude_clip ,join=st_within,left=TRUE, largest=TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  rename(obj_geb= objektart)

sas_tra_activities_classified_sf = subset(sas_tra_activities_classified_sf,
                                  select = -c(uuid.x...9,uuid.y...11, uuid.x...13, uuid.y...15,uuid ))

###Create presence/absence information for objects ----
sas_tra_activities_classified_sf$recreation_b <- if_else(is.na(sas_tra_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf$recreation_n <- if_else(is.na(sas_tra_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf$recreation_s <- if_else(is.na(sas_tra_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE", 
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sas_tra_activities_classified_sf$recreation[is.na(sas_tra_activities_classified_sf$recreation)] <- "FALSE" 

sas_tra_activities_classified_sf$recreation<-as.logical(sas_tra_activities_classified_sf$recreation)

sas_tra_activities_classified_sf$pub_trans <- if_else(is.na(sas_tra_activities_classified_sf$obj_pub_trans== TRUE)
                                       , FALSE, TRUE)

sas_tra_activities_classified_sf$gebaeude <- if_else(is.na(sas_tra_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sas_tra_activities_classified_sf<-sas_tra_activities_classified_sf[,c(1:8,17:19)]

### Classification ----
sas_tra_classification <- sas_tra_activities_classified_sf |> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                    if_else(pub_trans == TRUE, "travel",
                    if_else(recreation == TRUE, "recreation", "travel"),NA)))

sas_tra_classification <- sas_tra_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sas_tra_classification <- sas_tra_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sas_tra_test_classification, dsn="CAMA_data/ cama_classification_results_saskia_training.gpkg")#export classification results

###Confusion matrix ----
sas_tra_classification <-na.omit(sas_tra_classification)
confus_sas_tra_cama <-conf_mat(data = sas_tra_classification, truth = Attribute_factor, 
                  estimate = activity_factor)

###Compute model accuracy ----
confusionMatrix(sas_tra_classification$Attribute_factor, sas_tra_classification$activity_factor)

##Workflow with Saskia's test- data ----
###read activity data  ----
sas_tes_activities_classified_sf <-read_csv("CAMA_data/activities_saskia_attributiert.csv")
sas_tes_activities_classified_sf$ts_POSIXct <-as.POSIXct(sas_tes_activities_classified_sf$ts_POSIXct)

sas_tes_activities_classified_sf <-st_as_sf(sas_tes_activities_classified_sf, 
                          coords = c("lon", "lat"), crs = 4326 , remove = FALSE) 

sas_tes_activities_classified_sf <- st_transform(sas_tes_activities_classified_sf, crs = 2056)

###Join with objects -----
sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, bodenbuf,join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_boden= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, nutzungsbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, strassenbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_pub_trans= objektart)

sas_tes_activities_classified_sf<-st_join(sas_tes_activities_classified_sf, gebaeude_clip , join=st_within,left=TRUE, largest=TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  rename(obj_geb= objektart)

sas_tes_activities_classified_sf = subset(sas_tes_activities_classified_sf,
                          select = -c(fid,uuid.x...10,uuid.y...12, uuid.x...14,
                                      uuid.y...16,uuid))

###Create presence/absence information for objects ----
sas_tes_activities_classified_sf$recreation_b <- if_else(is.na(sas_tes_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$recreation_n <- if_else(is.na(sas_tes_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$recreation_s <- if_else(is.na(sas_tes_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE",
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sas_tes_activities_classified_sf$recreation[is.na(sas_tes_activities_classified_sf$recreation)] <- "FALSE" 

sas_tes_activities_classified_sf$recreation<-as.logical(sas_tes_activities_classified_sf$recreation)

sas_tes_activities_classified_sf$pub_trans <- if_else(is.na(sas_tes_activities_classified_sf$obj_pub_trans== TRUE) , FALSE, TRUE)

sas_tes_activities_classified_sf$gebaeude <- if_else(is.na(sas_tes_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sas_tes_activities_classified_sf<-sas_tes_activities_classified_sf[,c(1:8,17:19)]

###Classification ----
sas_test_classification <- sas_tes_activities_classified_sf|> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                            if_else(pub_trans == TRUE, "travel",
                            if_else(recreation == TRUE, "recreation", "travel"),NA)))


sas_test_classification <- sas_test_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sas_test_classification <-sas_test_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sas_test_classification, dsn="CAMA_data/ cama__results_saskia_test.gpkg")#export classification results

###Confusion matrix ----
sas_test_classification<-na.omit(sas_test_classification)
confus_sas_tes_cama <-conf_mat(data = sas_test_classification, truth = Attribute_factor, estimate = activity_factor)

###Compute model accuracy ----
confusionMatrix(sas_test_classification$Attribute_factor, sas_test_classification$activity_factor)

##Workflow with Sarah's test- data ----
###Read activity data  ----
sar_tes_activities_classified_sf <-read_sf("CAMA_data/activities_sarah_classified.gpkg")

sar_tes_activities_classified_sf <- st_transform(sar_tes_activities_classified_sf, crs = 2056)

###Join with objects -----
sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, bodenbuf,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_boden= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, nutzungsbuf , join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_nutzung= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, strassenbuf ,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_strassen= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, oevbuf , join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_pub_trans= objektart)

sar_tes_activities_classified_sf<-st_join(sar_tes_activities_classified_sf, gebaeude_clip ,join=st_within,left=TRUE, largest=TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  rename(obj_geb= objektart)

sar_tes_activities_classified_sf = subset(sar_tes_activities_classified_sf, 
                                  select = -c(uuid.x...9,uuid.y...11, uuid.x...13,uuid.y...15,uuid ))

###Create presence/absence information for objects ----
sar_tes_activities_classified_sf$recreation_b <- if_else(is.na(sar_tes_activities_classified_sf$obj_boden == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$recreation_n <- if_else(is.na(sar_tes_activities_classified_sf$obj_nutzung == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$recreation_s <- if_else(is.na(sar_tes_activities_classified_sf$obj_strassen == TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf |> 
  mutate(recreation = case_when(recreation_b == TRUE ~ "TRUE", 
                                recreation_n == TRUE ~ "TRUE", 
                                recreation_s == TRUE ~ "TRUE"))

sar_tes_activities_classified_sf$recreation[is.na(sar_tes_activities_classified_sf$recreation)] <- "FALSE" 

sar_tes_activities_classified_sf$recreation<-as.logical(sar_tes_activities_classified_sf$recreation)

sar_tes_activities_classified_sf$pub_trans <- if_else(is.na(sar_tes_activities_classified_sf$obj_pub_trans== TRUE) , FALSE, TRUE)

sar_tes_activities_classified_sf$gebaeude <- if_else(is.na(sar_tes_activities_classified_sf$obj_geb) == TRUE , FALSE, TRUE)    
sar_tes_activities_classified_sf<-sar_tes_activities_classified_sf[,c(1:8,17:19)]

###Classification ----
sar_test_classification <- sar_tes_activities_classified_sf |> 
  mutate(activity = if_else(gebaeude == TRUE, "shopping", 
                    if_else(pub_trans == TRUE, "travel",
                    if_else(recreation == TRUE, "recreation", "travel"),NA)))


sar_test_classification <- sar_test_classification |> 
  mutate(activity_factor = as.factor(activity)) 
sar_test_classification <-sar_test_classification |> 
  mutate(Attribute_factor = as.factor(Attribut))#change character to factor for confusion matrix

#st_write(sar_test_classification, dsn="CAMA_data/ cama_classification_results_sarah.gpkg")#export sar_test_classification results

###Confusion matrix ----
sar_test_classification<-na.omit(sar_test_classification)
confus_sar_tes_cama <-conf_mat(data = sar_test_classification, truth = Attribute_factor, estimate = activity_factor)

###Compute model accuracy ----
confusionMatrix(sar_test_classification$Attribute_factor, sar_test_classification$activity_factor)
```

```{r CART classification, results='hide', fig.keep='none'}
#| code-summary: CART based classification

##Workflow based on Saskia's training- data ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects ----
activities_attributes <-read_csv("test_activities_with_attributes_new.csv")
activities_attributes$ts_POSIXct <-as.POSIXct(activities_attributes$ts_POSIXct)

activities_attributes_sf <-st_as_sf(activities_attributes,
                                    coords = c("lon","lat"), crs = 4326 ,
                                    remove = FALSE) 

activities_attributes_sf <- st_transform(activities_attributes_sf, crs = 2056)

activities_attributes_sf  <-activities_attributes_sf [,c(1:26,30:32)]
#remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sas_train<-st_join(activities_attributes_sf,sas_tra_classification,  largest=TRUE)#largest = TRUE to remove duplicates

activities_sas_train$Attribute_factor.x <-as.factor(activities_sas_train$Attribute_factor.x)

###Create tree ----
set.seed(6832)
model_sas_train <-rpart(Attribute_factor.x~ speedMean+stepMean+acceleration+ recreation + pub_trans + gebaeude, data=activities_sas_train, method= "class")

###Make prediction on original data for confusion matrix----
pred_model_sas_train<- predict(model_sas_train, type="class")
activities_sas_train$pred<-pred_model_sas_train

### Confusion matrix for training- data ---
confus_cart_train <-conf_mat(data =activities_sas_train, truth = Attribute_factor.x,estimate = pred)

### Compute model accuracy rate on Sakia's training data----
confusionMatrix(activities_sas_train$Attribute_factor.x, activities_sas_train$pred)

##Make predictions on the test data from Saskia ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects ----

activities_attributes_sas_test <-read_csv("activities_saskia_with_attributes_classified_new.csv")
activities_attributes_sas_test$ts_POSIXct <-as.POSIXct(activities_attributes_sas_test$ts_POSIXct)

activities_attributes_sf_sas_test <-st_as_sf(activities_attributes_sas_test,
                                  coords = c("lon","lat"), crs = 4326 ,
                                  remove = FALSE)

activities_attributes_sf_sas_test  <- st_transform(activities_attributes_sf_sas_test , crs = 2056)

activities_attributes_sf_sas_test <-activities_attributes_sf_sas_test[,c(1:27,31:33)]##remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sas_test<-st_join(activities_attributes_sf_sas_test ,sas_test_classification, largest=TRUE)#largest = TRUE to remove duplicates

activities_sas_test$Attribute_factor.x <-as.factor(activities_sas_test$Attribute_factor.x)

pred_model_sas_test<- model_sas_train |>  
  predict(activities_sas_test, type = "class")

activities_sas_test$pred <-pred_model_sas_test

### Confusion matrix for Sakia's test- data  ---
confus_cart_test_sas <-conf_mat(data = activities_sas_test,
                           truth = Attribute_factor.x, estimate = pred)

### Compute model accuracy rate on Sakia's test data----
confusionMatrix(activities_sas_test$Attribute_factor.x, activities_sas_test$pred)

##Make predictions on the test data from Sarah ----
###Import movement attributes and results of CAMA analysis and transform them into spatial objects

activities_attributes_sarah <-read_csv("activities_sarah_with_attributes_classified_new.csv")
activities_attributes_sarah$ts_POSIXct <-as.POSIXct(activities_attributes_sarah$ts_POSIXct)

activities_attributes_sarah_sf <-st_as_sf(activities_attributes_sarah,
                                          coords = c("lon","lat"),
                                          crs = 4326 , remove = FALSE) 

activities_attributes_sarah_sf <- st_transform(activities_attributes_sarah_sf,
                                               crs = 2056)

activities_attributes_sarah_sf <-activities_attributes_sarah_sf[,c(1:27,31:33)]##remove non-required columns

###Join data from CAMA and walking- attributes -----
activities_sar_test<-st_join(activities_attributes_sarah_sf,sar_test_classification)#no largest required as the activity-data does not create duplicates

activities_sar_test$Attribute_factor.x <-as.factor(activities_sar_test$Attribute_factor.x)

pred_model_sar_test<- model_sas_train |>  
  predict(activities_sar_test, type = "class")

activities_sar_test$pred <-pred_model_sar_test

### Confusion matrix for Sarah's test- data  ---
confus_cart_sar_test <-conf_mat(data = activities_sar_test, 
                           truth = Attribute_factor.x, estimate = pred)

### Compute model accuracy rate on Sarah's test data----
confusionMatrix(activities_sar_test$Attribute_factor.x, activities_sar_test$pred)

```


## Abstract

The main question this paper attempts to answer is whether it is possible to differentiate between the walking pattern from recreation, travel and shopping. Data is collected with the Strava App. Three classification approaches are developed, tested and evaluated. The first is based on the attributes speed, step length and acceleration. The second a is context aware movement analysis (CAMA). The third is a classification and regression tree (CART) algorithm that combines the attributes generated in the first and second approach. We can conclude that the attribute-based classification is not able to cleanly separate travel and recreation. For shopping, there was no clear pattern to be found, as the GPS signal is most often lost in the building. The CAMA approach performed best in classifying the activities from the same person. For multiple people, CART was able do differentiate best as the activities are classified using all data, not only the training data recorded by one person.

## Introduction

The aim of this paper is to test and compare different approaches to detect walking patterns. The different patterns that should be able to be distinguished are walking for recreational purposes, walking for commuting and shopping. The underlying idea is that the pattern of these walking types differs in their attributes and the environment that there are based in. It was already shown that a differentiation between transit and walking based on speed is possible [@kim_new_2012]. Also, outdoor walking activities are generally longer, more continuous and faster with the walking duration being the most important attribute for classification [@baroudi_classification_2024].\
Based on walking data tracked with the Strava App, we want to answer the following research questions: \
* How well it possible to derive the type of activity from movement data considering the attributes step length, speed, acceleration and visited locations? \
* Which type of analytical approach performs best in differentiating between the different activities? \
* How does the performance of the analytical concepts differ when applied on trajectories recorded by different people? \
The three approaches used in this study are classification based on attributes of the trajectory, CAMA-analysis and classification based on CART tree. The approaches are described in detail in the chapter material and methods. The following figure show the workflow for the three classification methods. \

```{r overall workflow, out.width= "100%", fig.cap="Overall workflow for the classification methods." }
knitr::include_graphics(here::here("overall.png"))
```

## Material and Methods

### Data

Data was collected with the Strava App from March to June 2024. The training data for the development of the algorithms consists of 12 trajectories recorded by Saskia in March and April 2024. There are 13'636 attributed points. Additional data was collected by Saskia and Sarah in June 2024 to enable testing of the developed algorithms. Additional three trajectories recorded from Saskia in 2023 are used to test the algorithms. The algorithms are tested separately on the data of both. This allows drawing a conclusion whether the algorithms are able to differentiate walking patterns from different people. Saskia's test data consists of 6 trajectories and a total of 8'559 attributed points. Sarah's test data consists of 12 trajectories and a total of 17'648 attributed points. The analysis is done using the software R [@R] and RStudio [@Rstudio]. 

### Preparation

The data exported from Strava has the format gpx. The single activities are read and written into a data frame. IDs and descriptions for recognition are assigned to the single activities. The activities are then combined to one data frame and turned into a spatial object, which is then exported for manual classification. In QGIS [@QGIS_software], every point is given either the attribute shopping, recreation or travel. The points are selected by location using an Open Street Map [@OpenStreetMap] basemap. The activities are then again imported, turned into a spatial object and reprojected to LV95.


### Attribute-based classification

For the attribute-based classification, the time-lag and distance between  locations are being calculated. These calculations are checked for outliers. As the time-lag between two consecutive locations is mostly 1 s, all intervals bigger than 5 s are removed. All step lengths of more than 5 m are removed. As attributes also depend on the spatial scale, the mean step is calculated using the two previous and two next points. The speed is deducted from this mean step. The segmentation is done as described in @laube_how_2011. The threshold for movement is set to 1.5 m. This is based on the assumption that this equals around 3 - 4 steps, which can be interpreted as movement. Additionally, acceleration was calculated. After the calculation of the attributes, a summary over all activities is done to find differences between activity types. This information was used to set the thresholds. Based on the established differences, a classification is done. The classification is verified using a confusion matrix and its statistic output. The thresholds for classification are adapted until the best possible classification is reached. The following figure shows the workflow for the attribute-based classification. \

```{r atribute-based workflow, out.width= "100%", fig.cap="Workflow for the attribute-based classification. Indicated thresholds are used to classify activities." }
knitr::include_graphics(here::here("abw.png"))
```


### CAMA classification

The context aware movement analysis (CAMA) based classification is derived from the context. The layers tlm_bb_bodenbedeckung, tlm_areale_nutzungsareal, tlm_strassen_strasse tlm_oev_haltestelle and tlm_bauten_gebaeude_footprint of the swissTLM3D [@bundesamt_fur_landestopografie_swisstlm3d_2022] are used. For the first three layers a buffer of 10 m, for the fourth a buffer of 50 m and for the last layer no buffer is calculated. The layer tlm_strassen_strasse was reduced to streets of several categories (Ausfahrt, Einfahrt, Zufahrt, 3 m Strasse, 1 m Weg, 2 m Weg, 1 m Wegfragment and 2 m Wegfragment). For each layer a spatial join to the trajectories is performed. Then all single locations are classified using the following set of rules.\

```{r cama workflow, out.width= "100%", fig.cap="Workflow for the CAMA classification. Information about the context is added to the activity data." }
knitr::include_graphics(here::here("cama.png"))
```

### Validation

Validation of the classification is based on methods described by [@rykiel_testing_1996]. Methods used are visualization, comparison between the different approaches, internal validation and sensitivity analysis. The following process is applied to all previously described classification approaches. For every classification approach and dataset, a confusion matrix is created and the associated statistics are calculated.\


```{r validation workflow, out.width= "100%", fig.cap="Workflow for the validation process. This workflow is applied on all three datasets." }
knitr::include_graphics(here::here("validation.png"))
```

### CART classification
Using a classification and regression tree (CART) algorithm in rpart [@therneau_rpart_2023] a decision tree is built. Default parameters for the training data with the attributes from the attribute-based classification and the location attributes from the CAMA classification are used. The decision-tree is then applied to Saskia’s test-data and Sarah’s test-data independently. 


## Results

### Attribute based classification

#### Training data Saskia

Despite extensive testing and adapting of the thresholds, it was not possible to correctly classify all activities. 49 out of 12'886 points could not be classified and were removed before computing the confusion matrix. 7'526 points were classified correctly, which leads to an overall accuracy of 59 %. Sensitivity is highest for the class travel (0.61). Detection rate is highest for travel (0.54).

```{r confusion matrix, message=FALSE, results='hide', fig.cap="Confusion matrix of the attribute-based classification for the training data."}
#| code-summary: confusion matrix training data
# plot confusion matrix for test data
autoplot(confus_training, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")
```

#### Activity data Saskia

30 out of 6826 points could not be classified and were removed before computing the confusion matrix. 2270 points were classified correctly, leading to an overall accuracy of the model for Saskia's activity data of 33 %. Again, sensitivity is highest for travel (0.34), as well as the detection rate (0.33).

```{r confusion matrix Saskias data, message=FALSE, results='hide', fig.cap="Confusion matrix of the attribute-based classification for the test data. "}
#| code-summary: Confusion matrix for Saskias data
# plot confusion matrix
autoplot(confus_saskia, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")
```

#### Activity data Sarah

51 out of 17'061 points could not be classified and were removed before computing the confusion matrix. For Sarah's activities, 3209 points are classified correctly. The accuracy of the model is 19 %. The sensitivity for recreation is highest (0.71), the detection rate is highest for travel (0.16).  95 % of the points are classified as travel.

```{r confusion matrix Sarahs data, message=FALSE, results='hide', fig.cap="Confusion matrix of the attribute-based classification for Sarah's test data."}
#| code-summary: Confusion matrix Sarahs data
# plot confusion matrix
autoplot(confus_sarah, type="heatmap")+
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

### CAMA based classification

#### Training data Saskia
The overall accuracy for Saskia’s training data is 71%. The sensitivity (0.80) and detection rate (0.46) are highest for travel.
```{r CAMA confusion matrix Saskias training data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CAMA-based classification for Saskia's training data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_sas_tra_cama, type="heatmap")+
  scale_fill_gradient(low="#FCF3CF",high = "#F1C40F")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

#### Activity data Saskia
For Saskia’s test data the overall accuracy is 82%. The sensitivity (0.83)  and the detection rate (0.59) are highest for recreation.
```{r CAMA confusion matrix Saskias test data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CAMA-based classification for Saskia's test data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_sas_tes_cama, type="heatmap")+
  scale_fill_gradient(low="#FCF3CF",high = "#F1C40F")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

#### Activity data Sarah
The overall accuracy for Sarah’s activity data is 59%. With a sensitivity of 0.86 shopping is the most sensitive category. On the other hand, the detection rate is highest for recreation (0.27).
```{r CAMA confusion matrix Sarahs test data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CAMA-based classification for Sarah's test data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_sar_tes_cama, type="heatmap")+
  scale_fill_gradient(low="#FCF3CF",high = "#F1C40F")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

### CART based classification

#### Training data Saskia

The classification decision tree uses stepMean from the attribute based classification as well as the attributes recreation and pub_trans from the CAMA based classification. 

```{r CART tree based on Saskias training data, message=FALSE, results='hide', fig.cap="Classification decision tree. For the diversions recreation and pub_trans the number 1 corresponds to TRUE and the number 0 to FALSE. Therefore, the left branch of the diversion recreation corresponds to sites, which had the attribute recreation in the CAMA analysis set to TRUE. The three numbers at each leaf correspond to the number of points of each of the attributed walking activity categories (recreation, shopping, travel)."}
# plot classification tree
plot(model_sas_train)
text(model_sas_train, cex=0.8,use.n = TRUE, xpd = TRUE)
```


For Saskia’s training data the overall accuracy is 77%. The most sensitive category is travel (0.79) and the highest detection rate is for travel (0.52).
```{r CART confusion matrix Saskias training data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CART-based classification for Saskia's training data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_cart_train, type="heatmap")+
  scale_fill_gradient(low="#FADBD8",high = "#E74C3C")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

#### Activity data Saskia
The overall accuracy is 55% for Saskia’s test data. Recreation has the highest sensitivity (0.81) and travel has the highest detection rate (0.31).
```{r CART confusion matrix Saskias test data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CART-based classification for Saskia's test data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_cart_test_sas, type="heatmap")+
  scale_fill_gradient(low="#FADBD8",high = "#E74C3C")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

#### Activity data Sarah
For Sarah’s test data the overall accuracy is 47%. Sensitivity (0.85) and detection rate (0.19) are highest for shopping.
```{r CART confusion matrix Sarahs test data, message=FALSE, results='hide', fig.cap="Confusion matrix of the CART-based classification for Sarah's test data."}
#| code-summary: Confusion matrix Saskias training data
# plot confusion matrix
autoplot(confus_cart_sar_test, type="heatmap")+
  scale_fill_gradient(low="#FADBD8",high = "#E74C3C")+
  theme(legend.position = "right")+
  labs(fill="frequency")

```

## Discussion
### Attribute based classification
Based on the calculated attributes step length, speed an acceleration, it was not possible to clearly distinguish between different walking purposes. Most difficult was the separation of recreation and travel. These activity types are very similar in their attributes. Generally, travel is faster than recreation, but there were some activities where the mean speed of a recreational walk was just as fast as those for travel purposes. Also, speed differs between people. This explains the higher misclassification rate with Sarah’s data. 
The points from shopping activity are less accurate, as the signal is often lost within the building. For classification it was assumed that speed and step length are either higher or lower than for the other purposes. It was not possible to find a clear pattern that represents shopping activity. 
It can be concluded that with this approach it is not possible to clearly differentiate between shopping, travelling and walking for recreation. More information is needed, especially to differentiate between travel and recreation. In addition, the walking attributes differ not only between people but also between individual trajectories, which makes it more difficult to differentiate. 

### CAMA based classification
The CAMA based classification performed well on Saskia’s training dataset as well as Sarah’s testing dataset but even better on Saskia’s testing dataset. That a testing dataset outperforms a training dataset is in this case due to CAMA using a set of rules, which were not derived from the training data but our overall perception of the objects and land use an activity is associated with. The differences in performance between the datasets stem from the amount of points belonging to the activity type shopping in each dataset, which again have lower accuracy. Also for this analysis no points have been removed in preprocessing. This reduced the amount of points associated with shopping, which again leads to a high inclusion of points with low accuracy. Whether the buffer sizes for this analysis have been optimal for this case remains unknown. As @lee_effects_2019 showed, the buffer size has a high influence on the classification on activity types. A sensitivity analysis on the influence of the buffer size would have extended the scope of this work but is recommend for a further study .

### CART based classification
In the CART based classification a diminishing accuracy from Saskia’s training data to Saskia’s testing data and Sarah’s testing data is present. This shows the importance of using a dataset of a sufficient size to build a model. Further the importance of using data from different people to build a model and catch individual characteristics and patterns is highlighted. 

### Overall discussion
It can be concluded that a classification based only on the attributes step length, speed and acceleration does not perform well. Additionally, context information like visited locations is needed. The CAMA approach outperformed the attribute-based as well as the CART approach for Saskia's data. The CART approach performed best on Sarah's data, which highlights the importance of training algorithms with data collected by different people. The lack of accuracy of points recorded indoors is the main reason for misclassification of shopping activities. The exported data did not contain any information on GPS accuracy. As described in a review by @pearson_systematic_2024, there is a lack in consensus for the optimal reporting of GPS data with studies often missing to report noise. This information could have been used to improve our analysis by not only providing information about the reliability of the data but also to identify locations within a building. As @kim_new_2012 showed, the number of satellites can be used to assess whether a point was documented within a building. Another option to improve the differentiation between indoor and outdoor activities could be the use of the walking period duration and continuity as in @baroudi_classification_2024 or record density and heading changes as described in @berjisian_evaluation_2022. This study shows that classification of activity data is possible but its quality relies on context information as well as a wide range of data collected by different people. 


## Appendix
### Statement of Authorship for Student Work at the School of Life Sciences and Facility Management
By submitting the enclosed Project the student affirms independent completion of the(ir) work without outside help.
The undersigned student declares that all printed and electronic sources used in the text and the bibliography are correctly indicated, i.e., that the work does not contain any plagiarism (no parts that have been taken in whole or in part from another's or his/her own text or from another's or his/her own work without clear identification and without stating the source). In the event of misconduct of any kind, Paragraph 39 and Paragraph 40 of the General Academic Regulations for Bachelor's and Master's degree programmes at the Zurich University of Applied Sciences (dated 29 January 2008) and the provisions of the Disciplinary Measures of the University Regulations shall apply.\

![signatures](signature.png)


### Wordcount

<!-- after installing the wordcountadding, remove the line "#| eval: false" -->

```{r}
wordcountaddin::word_count("index.qmd")
```
